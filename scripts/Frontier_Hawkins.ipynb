{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python helper file/script for MATLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell sets up the notebook to import numpy, seaborn, pandas, matplotlib etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook.\n",
    "\n",
    "# These lines import the Numpy, Pandas, Seaborn, Matplotlib modules.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing plotting libraries and styles\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# For Pandas to ignore FutureWarning displays\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell given below sets up MATLAB for the notebook\n",
    "Source: https://sehyoun.com/blog/20180904_using-matlab-with-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "import io\n",
    "import scipy.io\n",
    "from IPython.core.magic import register_cell_magic\n",
    "ip = get_ipython()\n",
    "\n",
    "out = io.StringIO()\n",
    "err = io.StringIO()\n",
    "\n",
    "# Setup matlab cell magic #\n",
    "@register_cell_magic\n",
    "def matlab_magic(line,cell):\n",
    "    out.truncate(0)\n",
    "    out.seek(0)\n",
    "    err.truncate(0)\n",
    "    err.truncate(0)\n",
    "    raw = '''{line}.eval(\"\"\"{cell}\"\"\", nargout=0, stdout=out, stderr=err)'''\n",
    "    ip.run_cell(raw.format(line=line, cell=cell))\n",
    "    print(out.getvalue())\n",
    "    print(err.getvalue())\n",
    "    \n",
    "# Starting a MATLAB engine called eng\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Change this to the file path on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the MMB.m as well as MMBOPT1.m and MMBOPT2.m folders to the MATLAB engine path\"\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2', nargout=0)\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2/MMB_OPTIONS', nargout=0)\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/scripts', nargout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important:\n",
    "The code below sets the coefficients and other data for the PID rule to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the coefficients table here:\n",
    "\n",
    "https://rishab231.github.io/img/coefficients.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets the coefficients of the monetary policy rule, there are 33 coefficients and len(coefficients) = 33\n",
    "coefficients = [0, 0, 0, 0, 1.5/4, 1.5/4, 1.5/4, 1.5/4, \n",
    "                0, 0, 0, 0, 0, 0.5, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "\n",
    "# Number of the model you want to chooose, please exclude 69-79, 19-22, 27, 59, 65, 68, 81, 97, 98\n",
    "modelNum = 1\n",
    "\n",
    "scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important:** \n",
    "The cell below runs the MMB.m file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng.MMB(nargout = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions defined to import data for:\n",
    "* 4 IRF: Impulse Response Function Variables (outputgap, inflation, interest, output) and `modelName`\n",
    "* All IRF Variables\n",
    "* 4 ACF: Autocorrelation Function Variables (outputgap, inflation, interest, output)\n",
    "* **Unconditional Variances**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelName():\n",
    "    irf_4 = pd.read_excel(\"../mmb-gui-mlab-2.3.2/OUTPUT/results.xls\", sheetname = \"IRF Mon. Pol. Shock      \")\n",
    "    irf_4 = irf_4.T\n",
    "    irf_headers = irf_4.iloc[0] # grab the first row for the header\n",
    "    irf_4 = irf_4[1:] # take the data less the header row\n",
    "    irf_4_stripped_headers = [myHeader.strip() for myHeader in np.array(irf_headers)] # removing trailing whitespaces\n",
    "    irf_4.columns = irf_4_stripped_headers\n",
    "    modelName = irf_4.columns.values[1]\n",
    "    return modelName\n",
    "\n",
    "def singleModel_irf4():\n",
    "    irf_4 = pd.read_excel(\"../mmb-gui-mlab-2.3.2/OUTPUT/results.xls\", sheetname = \"IRF Mon. Pol. Shock      \")\n",
    "    irf_4 = irf_4.T\n",
    "    irf_headers = irf_4.iloc[0] # grab the first row for the header\n",
    "    irf_4 = irf_4[1:] # take the data less the header row\n",
    "    irf_4_stripped_headers = [myHeader.strip() for myHeader in np.array(irf_headers)] # removing trailing whitespaces\n",
    "    irf_4.columns = irf_4_stripped_headers\n",
    "    modelName = irf_4.columns.values[1]\n",
    "    irf_4 = irf_4.iloc[:, [i for i in range(1, len(irf_4.columns.values), 2)]]\n",
    "    irf_4.columns = [\"OutputGap\", \"Inflation\", \"Interest\", \"Output\"]\n",
    "    irf_4 = irf_4.reset_index()\n",
    "    irf_4.index.name = \"Period\"\n",
    "    irf_4.drop('index', axis=1, inplace=True)\n",
    "    return irf_4\n",
    "\n",
    "def singleModel_allirf():\n",
    "    old_irf_df = pd.read_excel(\"../mmb-gui-mlab-2.3.2/OUTPUT/results.xls\", sheetname = \"all IRFs Mon. Pol. Shock\")\n",
    "    all_irf = old_irf_df.T\n",
    "    new_header = all_irf.iloc[0] # grab the first row for the header\n",
    "    all_irf = all_irf[1:] # take the data less the header row\n",
    "    stripped_headers = [myHeader.strip() for myHeader in np.array(new_header)] # removing trailing whitespaces\n",
    "    all_irf.columns = stripped_headers # set the header row as the df header\n",
    "    all_irf[\"c_t\"] = all_irf.index\n",
    "    all_irf.index = np.arange(0,21,1)\n",
    "    all_irf.index.name = \"Period\"\n",
    "\n",
    "    # This section rearranges the columns\n",
    "    n = len(list(all_irf.columns.values))\n",
    "    rearranged = [list(all_irf.columns.values)[-1]] + list(all_irf.columns.values)[:n-1]\n",
    "    all_irf = all_irf[rearranged]\n",
    "    return all_irf\n",
    "\n",
    "def singleModel_acf():\n",
    "    acf = pd.read_excel(\"../mmb-gui-mlab-2.3.2/OUTPUT/results.xls\", sheetname = \"ACF\")\n",
    "    acf = acf.T\n",
    "    acf_headers = acf.iloc[0] # grab the first row for the header\n",
    "    acf = acf[1:] # take the data less the header row\n",
    "    acf_stripped_headers = [myHeader.strip() for myHeader in np.array(acf_headers)] # removing trailing whitespaces\n",
    "    acf.columns = acf_stripped_headers\n",
    "    acf = acf.iloc[:, [i for i in range(0, len(acf.columns.values), 2)]]\n",
    "    acf.columns = [\"OutputGap\", \"Inflation\", \"Interest\", \"Output\"]\n",
    "    acf = acf.reset_index()\n",
    "    acf.index.name = \"Period\"\n",
    "    acf.drop('index', axis=1, inplace=True)\n",
    "    return acf\n",
    "\n",
    "def unconditionalVariances():\n",
    "    var4 = pd.read_csv(\"../mmb-gui-mlab-2.3.2/OUTPUT/variances.csv\", names=[\"interest\", \"inflation\", \"outputgap\", \"output\"])\n",
    "    return var4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputGap</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.823109</td>\n",
       "      <td>-0.0181086</td>\n",
       "      <td>0.561283</td>\n",
       "      <td>-0.823109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0414876</td>\n",
       "      <td>-0.0161234</td>\n",
       "      <td>-0.00344134</td>\n",
       "      <td>0.0414876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        OutputGap  Inflation    Interest     Output\n",
       "Period                                             \n",
       "0               0          0           0          0\n",
       "1       -0.823109 -0.0181086    0.561283  -0.823109\n",
       "2       0.0414876 -0.0161234 -0.00344134  0.0414876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleModel_irf4().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputGap</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.661577</td>\n",
       "      <td>0.716781</td>\n",
       "      <td>0.755958</td>\n",
       "      <td>0.730718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375576</td>\n",
       "      <td>0.478675</td>\n",
       "      <td>0.590853</td>\n",
       "      <td>0.520949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       OutputGap Inflation  Interest    Output\n",
       "Period                                        \n",
       "0              1         1         1         1\n",
       "1       0.661577  0.716781  0.755958  0.730718\n",
       "2       0.375576  0.478675  0.590853  0.520949"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleModel_acf().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>inflation</th>\n",
       "      <th>outputgap</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079687</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.38901</td>\n",
       "      <td>1.1702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   interest  inflation  outputgap  output\n",
       "0  0.079687    0.06191    0.38901  1.1702"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unconditionalVariances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calculating `unconditionalVariances` for different models, in an array of `modelNums`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myVariance(modelNums):\n",
    "    variances = dict()\n",
    "    for modelNum in modelNums:\n",
    "        eng.MMB(nargout = 0)\n",
    "        scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum))\n",
    "        modelName = getModelName()\n",
    "        variances[modelName] = unconditionalVariances().values.tolist()[0]\n",
    "    return variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating unconditional variances for different rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NK_RW97': [0.079687, 0.06191, 0.38900999999999997, 1.1702]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVariance(np.arange(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NK_RW97': [0.079687, 0.06191, 0.38900999999999997, 1.1702]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = [0, 0, 0, 0, 1.5/4, 1.5/4, 1.5/4, 1.5/4, \n",
    "                0, 0, 0, 0, 0, 0.5, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum))\n",
    "eng.MMB(nargout = 0)\n",
    "myVariance(np.arange(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID_loss_array = []\n",
    "PID_final_inflation_variance = 0\n",
    "PID_final_output_gap_variance = 0\n",
    "PID_final_interest_rate_variance = 0\n",
    "currentPIDLoss = 0\n",
    "\n",
    "def myPID(selected_coeff, lambdaVal, coeffInterest, modelNum, VarTarget):\n",
    "    global PID_final_inflation_variance\n",
    "    global PID_final_output_gap_variance\n",
    "    global PID_final_interest_rate_variance\n",
    "    global currentPIDLoss\n",
    "\n",
    "    coefficients = [1, 0, 0, 0, \n",
    "                    abs(selected_coeff[0])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    selected_coeff[1]/4, \n",
    "                    0, 0, 0, 0, \n",
    "                    selected_coeff[2], selected_coeff[3], 0, \n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "    scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum)) # Input for MMB.\n",
    "    eng.MMB(nargout = 0) # Run MMB\n",
    "    \n",
    "    interest_rate_variance = unconditionalVariances()['interest'][0]\n",
    "    inflation_variance = unconditionalVariances()['inflation'][0]\n",
    "    output_gap_variance = unconditionalVariances()['outputgap'][0]\n",
    "    \n",
    "    PID_final_inflation_variance = inflation_variance\n",
    "    PID_final_output_gap_variance = output_gap_variance\n",
    "    PID_final_interest_rate_variance = interest_rate_variance\n",
    "    \n",
    "    PID_loss = ((interest_rate_variance / VarTarget[0]) * coeffInterest \n",
    "                + (inflation_variance / VarTarget[1]) * lambdaVal\n",
    "                + (output_gap_variance/ VarTarget[2]) * (1 - lambdaVal) )\n",
    "    \n",
    "    print(\"Total PID Loss\", PID_loss, \"lambda=\", lambdaVal)\n",
    "    PID_loss_array.append(PID_loss)\n",
    "    currentPIDLoss = PID_loss\n",
    "    \n",
    "    return PID_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFrontier(modelNum):\n",
    "    taylor_coeff = [1.5, 0, 0.5, 0] # Taylor rule coefficients.\n",
    "    hamilton_coeff = [1.42,-1.20,0.5,-0.48] # Hamilton fit coefficients.\n",
    "    weights = np.arange(0, 1.01, .25) # Values for lambda\n",
    "    interest_coeff = 1.0 # Coefficient on the interest_rate loss term\n",
    "    VarTarget = [1.0, 1.0, 1.0] # Normalizing coefficients for PID_loss on inital pass.\n",
    "    \n",
    "    mylambdatemp = 0.0 # Something to put in the lambda variable when calculating the target rate variance.\n",
    "    myfoo = myPID(taylor_coeff, mylambdatemp, interest_coeff, modelNum, VarTarget) # Run with Hamilton coefficients to generate target rate variance.\n",
    "    VarTarget[0] = unconditionalVariances()['interest'][0] # Assign normalizing variances.\n",
    "    VarTarget[1] = unconditionalVariances()['inflation'][0] # Assign normalizing variances.\n",
    "    VarTarget[2] = unconditionalVariances()['outputgap'][0] # Assign normalizing variances.\n",
    "    print(\"The normalizing variances are\", VarTarget) # Print normalizing variance.\n",
    "    \n",
    "    PID_variance_tuples = []\n",
    "    PID_loss_of_lambda = []\n",
    "    PID_coefficients_array = []\n",
    "\n",
    "    # Starting with the Taylor rule coefficients\n",
    "    current_coeff = taylor_coeff\n",
    "\n",
    "    for lambda_value in weights:\n",
    "        PID_result = scipy.optimize.minimize(myPID, current_coeff, \\\n",
    "                                    args=(lambda_value, interest_coeff, modelNum, VarTarget), method='Nelder-Mead')\n",
    "        current_coeff = PID_result.x\n",
    "        PID_coefficients_array.append(current_coeff)\n",
    "        PID_variance_tuples.append((PID_final_inflation_variance, PID_final_output_gap_variance, PID_final_interest_rate_variance))\n",
    "        PID_loss_of_lambda.append(currentPIDLoss)\n",
    "    \n",
    "    # The code below plots the frontier\n",
    "    nameOfModel = getModelName()\n",
    "    \n",
    "    SD_inflation_scatter = np.asarray([np.sqrt(tpl[0]) for tpl in PID_variance_tuples])\n",
    "    SD_output_gap_scatter = np.asarray([np.sqrt(tpl[1]) for tpl in PID_variance_tuples])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(SD_inflation_scatter, SD_output_gap_scatter, color=\"red\")\n",
    "\n",
    "    for i in range(0, len(weights)):\n",
    "        ax.annotate(weights[i], (SD_inflation_scatter[i], SD_output_gap_scatter[i]))\n",
    "\n",
    "    ax.set_xlabel('$\\sigma_{\\pi}$', fontsize=10)\n",
    "    ax.set_ylabel('$\\sigma_{y}$', fontsize=10)\n",
    "    ax.set_title('Policy Frontiers for Different Weights, Model: ' + nameOfModel, fontsize=14)\n",
    "    \n",
    "    # The code below saves the results of the all the optimizations with the appropriate lambdas into a DataFrame\n",
    "    results = dict()\n",
    "    results['lambdas'] = weights\n",
    "    results['variance_tuples'] = PID_variance_tuples\n",
    "    results['losses'] = PID_loss_of_lambda\n",
    "    results['coefficients'] = PID_coefficients_array\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    # Saving DataFrame to nameOfModel.csv file\n",
    "    results_df.to_csv(nameOfModel + \".csv\", index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 27.1216 lambda= 0.0\n",
      "The normalizing variances are [20.449, 0.74862, 6.6726]\n",
      "Total PID Loss 2.0 lambda= 0.0\n",
      "Total PID Loss 2.085503613994411 lambda= 0.0\n",
      "Total PID Loss 2.000217697588576 lambda= 0.0\n",
      "Total PID Loss 1.972914152128113 lambda= 0.0\n",
      "Total PID Loss 1.9998130577180606 lambda= 0.0\n",
      "Total PID Loss 1.9031489435403026 lambda= 0.0\n",
      "Total PID Loss 1.8161456878955997 lambda= 0.0\n",
      "Total PID Loss 1.893793227631496 lambda= 0.0\n",
      "Total PID Loss 1.8438243674802366 lambda= 0.0\n",
      "Total PID Loss 1.7723049455323114 lambda= 0.0\n",
      "Total PID Loss 1.6725083610497897 lambda= 0.0\n",
      "Total PID Loss 1.644750249738733 lambda= 0.0\n",
      "Total PID Loss 1.4969990398701154 lambda= 0.0\n",
      "Total PID Loss 1.5294719517810966 lambda= 0.0\n",
      "Total PID Loss 1.4282228300405966 lambda= 0.0\n",
      "Total PID Loss 1.2583828767867282 lambda= 0.0\n",
      "Total PID Loss 1.2284408646073688 lambda= 0.0\n",
      "Total PID Loss 1.0575312635552099 lambda= 0.0\n",
      "Total PID Loss 1.0575312635552099 lambda= 0.0\n"
     ]
    },
    {
     "ename": "MatlabExecutionError",
     "evalue": "\n  File /Users/rishabsrivastava/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2/MMB.m, line 15, in MMB\n'../scripts/variables.mat' is not found in the current folder or on the MATLAB path, but exists in:\n    /Users/rishabsrivastava/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2\n\nChange the MATLAB current folder or add its folder to the MATLAB path.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMatlabExecutionError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d745224cbd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm43results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateFrontier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-d54eeb4ad9d5>\u001b[0m in \u001b[0;36mcreateFrontier\u001b[0;34m(modelNum)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlambda_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         PID_result = scipy.optimize.minimize(myPID, current_coeff, \\\n\u001b[0;32m---> 24\u001b[0;31m                                     args=(lambda_value, interest_coeff, modelNum, VarTarget), method='Nelder-Mead')\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mcurrent_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPID_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mPID_coefficients_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    473\u001b[0m                       callback=callback, **options)\n\u001b[1;32m    474\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nelder-mead'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'powell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_powell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, **unknown_options)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mxbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxbar\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mfxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mdoshrink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-d2039cdb8612>\u001b[0m in \u001b[0;36mmyPID\u001b[0;34m(selected_coeff, lambdaVal, coeffInterest, modelNum, VarTarget)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n\u001b[1;32m     22\u001b[0m     \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variables.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelNumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Input for MMB.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0meng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMMB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnargout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run MMB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minterest_rate_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconditionalVariances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matlab/engine/matlabengine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             return FutureResult(self._engine(), future, nargs, _stdout,\n\u001b[0;32m---> 71\u001b[0;31m                                 _stderr, feval=True).result()\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__validate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matlab/engine/futureresult.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TimeoutCannotBeNegative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matlab/engine/fevalfuture.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MatlabFunctionTimeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpythonengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFEvalResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nargout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMatlabExecutionError\u001b[0m: \n  File /Users/rishabsrivastava/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2/MMB.m, line 15, in MMB\n'../scripts/variables.mat' is not found in the current folder or on the MATLAB path, but exists in:\n    /Users/rishabsrivastava/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2\n\nChange the MATLAB current folder or add its folder to the MATLAB path.\n"
     ]
    }
   ],
   "source": [
    "m43results = createFrontier(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambdas</th>\n",
       "      <th>variance_tuples</th>\n",
       "      <th>losses</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>(3.0092, 3.5886, 2.8254)</td>\n",
       "      <td>0.675979</td>\n",
       "      <td>[0.06039161776872047, 0.0033466406104729948, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>(1.7523, 4.0998, 2.804)</td>\n",
       "      <td>0.967330</td>\n",
       "      <td>[0.1413716149389495, 0.005947307714789774, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250</td>\n",
       "      <td>(1.4176, 4.6054, 3.1825)</td>\n",
       "      <td>1.146682</td>\n",
       "      <td>[0.20236481178732924, 0.005615080522340945, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375</td>\n",
       "      <td>(1.2347, 5.0993, 3.6878)</td>\n",
       "      <td>1.276464</td>\n",
       "      <td>[0.261201184096281, 0.005545794502150973, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500</td>\n",
       "      <td>(1.1176, 5.5915, 4.2367)</td>\n",
       "      <td>1.372613</td>\n",
       "      <td>[0.3180800423566656, 0.005708351424169934, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.625</td>\n",
       "      <td>(1.0270000000000001, 6.1062, 4.9373)</td>\n",
       "      <td>1.442024</td>\n",
       "      <td>[0.38204575082460823, 0.006218691213010901, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.750</td>\n",
       "      <td>(0.9573799999999999, 6.6739999999999995, 5.7016)</td>\n",
       "      <td>1.488018</td>\n",
       "      <td>[0.44839500568777135, 0.006421235084380261, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>(0.9004700000000001, 7.3096, 6.5998)</td>\n",
       "      <td>1.512162</td>\n",
       "      <td>[0.5212184279557517, 0.006420086084443955, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000</td>\n",
       "      <td>(0.8519100000000001, 8.0135, 7.7077)</td>\n",
       "      <td>1.514897</td>\n",
       "      <td>[0.6046622608093379, 0.006609206638598408, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambdas                                   variance_tuples    losses  \\\n",
       "0    0.000                          (3.0092, 3.5886, 2.8254)  0.675979   \n",
       "1    0.125                           (1.7523, 4.0998, 2.804)  0.967330   \n",
       "2    0.250                          (1.4176, 4.6054, 3.1825)  1.146682   \n",
       "3    0.375                          (1.2347, 5.0993, 3.6878)  1.276464   \n",
       "4    0.500                          (1.1176, 5.5915, 4.2367)  1.372613   \n",
       "5    0.625              (1.0270000000000001, 6.1062, 4.9373)  1.442024   \n",
       "6    0.750  (0.9573799999999999, 6.6739999999999995, 5.7016)  1.488018   \n",
       "7    0.875              (0.9004700000000001, 7.3096, 6.5998)  1.512162   \n",
       "8    1.000              (0.8519100000000001, 8.0135, 7.7077)  1.514897   \n",
       "\n",
       "                                        coefficients  \n",
       "0  [0.06039161776872047, 0.0033466406104729948, 0...  \n",
       "1  [0.1413716149389495, 0.005947307714789774, 0.1...  \n",
       "2  [0.20236481178732924, 0.005615080522340945, 0....  \n",
       "3  [0.261201184096281, 0.005545794502150973, 0.11...  \n",
       "4  [0.3180800423566656, 0.005708351424169934, 0.1...  \n",
       "5  [0.38204575082460823, 0.006218691213010901, 0....  \n",
       "6  [0.44839500568777135, 0.006421235084380261, 0....  \n",
       "7  [0.5212184279557517, 0.006420086084443955, 0.1...  \n",
       "8  [0.6046622608093379, 0.006609206638598408, 0.1...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m43results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 5.258900000000001 lambda= 0.0\n",
      "The normalizing variances are [3.5398, 0.43228999999999995, 1.7191]\n",
      "Total PID Loss 2.0 lambda= 0.0\n",
      "Total PID Loss 2.1325749549545883 lambda= 0.0\n",
      "Total PID Loss 2.0003423414143295 lambda= 0.0\n",
      "Total PID Loss 1.9518381910201397 lambda= 0.0\n",
      "Total PID Loss 1.9996825695584808 lambda= 0.0\n",
      "Total PID Loss 1.849193598171511 lambda= 0.0\n",
      "Total PID Loss 1.7171246782669558 lambda= 0.0\n",
      "Total PID Loss 1.8321075844162436 lambda= 0.0\n",
      "Total PID Loss 1.7531510063535092 lambda= 0.0\n",
      "Total PID Loss 1.6403605320281769 lambda= 0.0\n",
      "Total PID Loss 1.483430724188486 lambda= 0.0\n",
      "Total PID Loss 1.455959858465972 lambda= 0.0\n",
      "Total PID Loss 1.245994982263877 lambda= 0.0\n",
      "Total PID Loss 1.284411820478939 lambda= 0.0\n",
      "Total PID Loss 1.1453597693833208 lambda= 0.0\n",
      "Total PID Loss 0.9268443180282917 lambda= 0.0\n",
      "Total PID Loss 0.878943484806783 lambda= 0.0\n",
      "Total PID Loss 0.7884131744500456 lambda= 0.0\n",
      "Total PID Loss 0.763918487182109 lambda= 0.0\n",
      "Total PID Loss 5.1565361990221446 lambda= 0.0\n",
      "Total PID Loss 0.9967773292853203 lambda= 0.0\n",
      "Total PID Loss 3.1563997077940753 lambda= 0.0\n",
      "Total PID Loss 0.9107502401807901 lambda= 0.0\n",
      "Total PID Loss 1.0947301767955355 lambda= 0.0\n",
      "Total PID Loss 0.771658052494228 lambda= 0.0\n",
      "Total PID Loss 1.0493181070228175 lambda= 0.0\n",
      "Total PID Loss 0.8213892238388665 lambda= 0.0\n",
      "Total PID Loss 1.2233938408302523 lambda= 0.0\n",
      "Total PID Loss 0.8077915886390438 lambda= 0.0\n",
      "Total PID Loss 0.8885940515462865 lambda= 0.0\n",
      "Total PID Loss 0.7798391502150197 lambda= 0.0\n",
      "Total PID Loss 0.9015968451215095 lambda= 0.0\n",
      "Total PID Loss 0.7738594196650772 lambda= 0.0\n",
      "Total PID Loss 0.7756582104625631 lambda= 0.0\n",
      "Total PID Loss 0.7623299943602504 lambda= 0.0\n",
      "Total PID Loss 0.7956606948879958 lambda= 0.0\n",
      "Total PID Loss 0.8324186164565661 lambda= 0.0\n",
      "Total PID Loss 0.760860339975899 lambda= 0.0\n",
      "Total PID Loss 0.7966503659825996 lambda= 0.0\n",
      "Total PID Loss 0.7628618948189413 lambda= 0.0\n",
      "Total PID Loss 0.7570274179674961 lambda= 0.0\n",
      "Total PID Loss 0.765301850245867 lambda= 0.0\n",
      "Total PID Loss 0.7644567084776505 lambda= 0.0\n",
      "Total PID Loss 0.7587307714248441 lambda= 0.0\n",
      "Total PID Loss 0.750146347651568 lambda= 0.0\n",
      "Total PID Loss 0.7453285333010473 lambda= 0.0\n",
      "Total PID Loss 0.7618821937007241 lambda= 0.0\n",
      "Total PID Loss 0.7556922525993743 lambda= 0.0\n",
      "Total PID Loss 0.7476620106290828 lambda= 0.0\n",
      "Total PID Loss 0.7418525226434565 lambda= 0.0\n",
      "Total PID Loss 0.7374409719306826 lambda= 0.0\n",
      "Total PID Loss 0.7327164464536561 lambda= 0.0\n",
      "Total PID Loss 0.7265322158629282 lambda= 0.0\n",
      "Total PID Loss 0.7285482551902075 lambda= 0.0\n",
      "Total PID Loss 0.7086274677125347 lambda= 0.0\n",
      "Total PID Loss 0.6868993550587099 lambda= 0.0\n",
      "Total PID Loss 0.6811189369409395 lambda= 0.0\n",
      "Total PID Loss 0.6459644813995752 lambda= 0.0\n",
      "Total PID Loss 0.7137414168190639 lambda= 0.0\n",
      "Total PID Loss 0.6297527275937648 lambda= 0.0\n",
      "Total PID Loss 0.582127174014811 lambda= 0.0\n",
      "Total PID Loss 0.5413696821264228 lambda= 0.0\n",
      "Total PID Loss 5.202094486131756 lambda= 0.0\n",
      "Total PID Loss 0.6967165825988025 lambda= 0.0\n",
      "Total PID Loss 0.6337175686421207 lambda= 0.0\n",
      "Total PID Loss 1.4975424686402337 lambda= 0.0\n",
      "Total PID Loss 0.6432712405877105 lambda= 0.0\n",
      "Total PID Loss 0.5755343171960854 lambda= 0.0\n",
      "Total PID Loss 1.609507146468228 lambda= 0.0\n",
      "Total PID Loss 0.6087391432799126 lambda= 0.0\n",
      "Total PID Loss 0.4967113318869927 lambda= 0.0\n",
      "Total PID Loss 0.67080544959468 lambda= 0.0\n",
      "Total PID Loss 0.6707576876561954 lambda= 0.0\n",
      "Total PID Loss 0.5778032346954889 lambda= 0.0\n",
      "Total PID Loss 0.511056699540003 lambda= 0.0\n",
      "Total PID Loss 0.7250289032852769 lambda= 0.0\n",
      "Total PID Loss 0.5514164864903336 lambda= 0.0\n",
      "Total PID Loss 0.5021067606894654 lambda= 0.0\n",
      "Total PID Loss 0.4520005695129218 lambda= 0.0\n",
      "Total PID Loss 0.6237658229991687 lambda= 0.0\n",
      "Total PID Loss 0.5010766572076837 lambda= 0.0\n",
      "Total PID Loss 0.5798603431935047 lambda= 0.0\n",
      "Total PID Loss 0.47825010113191063 lambda= 0.0\n",
      "Total PID Loss 0.4432748304036683 lambda= 0.0\n",
      "Total PID Loss 15.78543863084482 lambda= 0.0\n",
      "Total PID Loss 0.5182493340008117 lambda= 0.0\n",
      "Total PID Loss 0.452750993383173 lambda= 0.0\n",
      "Total PID Loss 0.46885850895777315 lambda= 0.0\n",
      "Total PID Loss 0.5703316048820037 lambda= 0.0\n",
      "Total PID Loss 0.45522681344610405 lambda= 0.0\n",
      "Total PID Loss 0.48654289167486064 lambda= 0.0\n",
      "Total PID Loss 0.43474358076899716 lambda= 0.0\n",
      "Total PID Loss 0.4428399852905134 lambda= 0.0\n",
      "Total PID Loss 0.46249776012410343 lambda= 0.0\n",
      "Total PID Loss 0.4433218046532159 lambda= 0.0\n",
      "Total PID Loss 0.4450838300494326 lambda= 0.0\n",
      "Total PID Loss 0.43411474640555725 lambda= 0.0\n",
      "Total PID Loss 0.4778143412196038 lambda= 0.0\n",
      "Total PID Loss 0.43731719271008607 lambda= 0.0\n",
      "Total PID Loss 0.4615248966644896 lambda= 0.0\n",
      "Total PID Loss 0.4359864884421615 lambda= 0.0\n",
      "Total PID Loss 0.4445295572726731 lambda= 0.0\n",
      "Total PID Loss 0.43607140217396234 lambda= 0.0\n",
      "Total PID Loss 0.43788113013578633 lambda= 0.0\n",
      "Total PID Loss 0.4352458700527245 lambda= 0.0\n",
      "Total PID Loss 0.43836640512155534 lambda= 0.0\n",
      "Total PID Loss 0.4345175876808809 lambda= 0.0\n",
      "Total PID Loss 0.4359757025611639 lambda= 0.0\n",
      "Total PID Loss 0.43451530232631347 lambda= 0.0\n",
      "Total PID Loss 0.4343312166625936 lambda= 0.0\n",
      "Total PID Loss 0.43644327539126615 lambda= 0.0\n",
      "Total PID Loss 0.4340434831769458 lambda= 0.0\n",
      "Total PID Loss 0.4349719097435374 lambda= 0.0\n",
      "Total PID Loss 0.4340694430760673 lambda= 0.0\n",
      "Total PID Loss 0.43502201838144183 lambda= 0.0\n",
      "Total PID Loss 0.43405569791808324 lambda= 0.0\n",
      "Total PID Loss 0.4344819912334607 lambda= 0.0\n",
      "Total PID Loss 0.43398304001023 lambda= 0.0\n",
      "Total PID Loss 0.4343596334780981 lambda= 0.0\n",
      "Total PID Loss 0.433937679986462 lambda= 0.0\n",
      "Total PID Loss 0.4340522693110727 lambda= 0.0\n",
      "Total PID Loss 0.43437383991387546 lambda= 0.0\n",
      "Total PID Loss 0.43392694849910507 lambda= 0.0\n",
      "Total PID Loss 0.4339921053431353 lambda= 0.0\n",
      "Total PID Loss 0.43426980903582496 lambda= 0.0\n",
      "Total PID Loss 0.4339291661492013 lambda= 0.0\n",
      "Total PID Loss 0.4340857751364459 lambda= 0.0\n",
      "Total PID Loss 0.433927189244373 lambda= 0.0\n",
      "Total PID Loss 0.4340021834494783 lambda= 0.0\n",
      "Total PID Loss 0.43392467037511223 lambda= 0.0\n",
      "Total PID Loss 0.433952788600752 lambda= 0.0\n",
      "Total PID Loss 0.43391810567070005 lambda= 0.0\n",
      "Total PID Loss 0.4340026420979718 lambda= 0.0\n",
      "Total PID Loss 0.4339103056883499 lambda= 0.0\n",
      "Total PID Loss 0.43393019338378824 lambda= 0.0\n",
      "Total PID Loss 0.43391414578078763 lambda= 0.0\n",
      "Total PID Loss 0.43391530727399846 lambda= 0.0\n",
      "Total PID Loss 0.43391739181578953 lambda= 0.0\n",
      "Total PID Loss 0.4339178660757508 lambda= 0.0\n",
      "Total PID Loss 0.43391061298777034 lambda= 0.0\n",
      "Total PID Loss 0.4339136110469297 lambda= 0.0\n",
      "Total PID Loss 0.43392930648808103 lambda= 0.0\n",
      "Total PID Loss 0.4339079210777129 lambda= 0.0\n",
      "Total PID Loss 0.43391140062740813 lambda= 0.0\n",
      "Total PID Loss 0.433921051636856 lambda= 0.0\n",
      "Total PID Loss 0.43390824168796394 lambda= 0.0\n",
      "Total PID Loss 0.4339172780985708 lambda= 0.0\n",
      "Total PID Loss 0.43390854898738446 lambda= 0.0\n",
      "Total PID Loss 0.43391412523938255 lambda= 0.0\n",
      "Total PID Loss 0.4339093305467662 lambda= 0.0\n",
      "Total PID Loss 0.43390812797074524 lambda= 0.0\n",
      "Total PID Loss 0.4339143188215844 lambda= 0.0\n",
      "Total PID Loss 0.4339040543636141 lambda= 0.0\n",
      "Total PID Loss 0.43391144055989966 lambda= 0.0\n",
      "Total PID Loss 0.43390923014037813 lambda= 0.0\n",
      "Total PID Loss 0.4339088962192965 lambda= 0.0\n",
      "Total PID Loss 0.43391041217499393 lambda= 0.0\n",
      "Total PID Loss 0.43390756053497037 lambda= 0.0\n",
      "Total PID Loss 0.4339077141846806 lambda= 0.0\n",
      "Total PID Loss 0.4339080214841011 lambda= 0.0\n",
      "Total PID Loss 0.5900721886196635 lambda= 0.125\n",
      "Total PID Loss 0.5821501130123715 lambda= 0.125\n",
      "Total PID Loss 0.5891660347931125 lambda= 0.125\n",
      "Total PID Loss 0.5966747423371942 lambda= 0.125\n",
      "Total PID Loss 0.5915696585653766 lambda= 0.125\n",
      "Total PID Loss 0.5804651976737991 lambda= 0.125\n",
      "Total PID Loss 0.5740000269652155 lambda= 0.125\n",
      "Total PID Loss 0.5764686560372514 lambda= 0.125\n",
      "Total PID Loss 0.5723719012642005 lambda= 0.125\n",
      "Total PID Loss 0.566886611756314 lambda= 0.125\n",
      "Total PID Loss 0.5652646128713557 lambda= 0.125\n",
      "Total PID Loss 0.5618277840065251 lambda= 0.125\n",
      "Total PID Loss 0.5610964169130109 lambda= 0.125\n",
      "Total PID Loss 0.5629475732728204 lambda= 0.125\n",
      "Total PID Loss 0.5615426061165733 lambda= 0.125\n",
      "Total PID Loss 0.5664613137993677 lambda= 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 0.5717626413830685 lambda= 0.125\n",
      "Total PID Loss 0.5623192442019845 lambda= 0.125\n",
      "Total PID Loss 0.5692139982625324 lambda= 0.125\n",
      "Total PID Loss 0.562485774884334 lambda= 0.125\n",
      "Total PID Loss 0.5635547840892747 lambda= 0.125\n",
      "Total PID Loss 0.5615755049753952 lambda= 0.125\n",
      "Total PID Loss 0.5620462059612752 lambda= 0.125\n",
      "Total PID Loss 0.5614689173093648 lambda= 0.125\n",
      "Total PID Loss 0.5613085253110914 lambda= 0.125\n",
      "Total PID Loss 0.5606904309458809 lambda= 0.125\n",
      "Total PID Loss 0.5602649997354336 lambda= 0.125\n",
      "Total PID Loss 0.5602344834968327 lambda= 0.125\n",
      "Total PID Loss 0.5598218677754695 lambda= 0.125\n",
      "Total PID Loss 0.559654465390153 lambda= 0.125\n",
      "Total PID Loss 0.559397272750544 lambda= 0.125\n",
      "Total PID Loss 0.5611982415829946 lambda= 0.125\n",
      "Total PID Loss 0.5602094473679571 lambda= 0.125\n",
      "Total PID Loss 0.5584298309919036 lambda= 0.125\n",
      "Total PID Loss 0.5573895008709054 lambda= 0.125\n",
      "Total PID Loss 0.5597631862115937 lambda= 0.125\n",
      "Total PID Loss 0.5574903268078353 lambda= 0.125\n",
      "Total PID Loss 0.5577158963792319 lambda= 0.125\n",
      "Total PID Loss 0.5572312379019351 lambda= 0.125\n",
      "Total PID Loss 0.5584058747464642 lambda= 0.125\n",
      "Total PID Loss 0.556064161810692 lambda= 0.125\n",
      "Total PID Loss 0.557679707094159 lambda= 0.125\n",
      "Total PID Loss 0.5586155307029136 lambda= 0.125\n",
      "Total PID Loss 0.5569100708627649 lambda= 0.125\n",
      "Total PID Loss 0.5563520092926146 lambda= 0.125\n",
      "Total PID Loss 0.555996222119536 lambda= 0.125\n",
      "Total PID Loss 0.5564674859965919 lambda= 0.125\n",
      "Total PID Loss 0.5556571670622099 lambda= 0.125\n",
      "Total PID Loss 0.5559510876272142 lambda= 0.125\n",
      "Total PID Loss 0.5580457058466636 lambda= 0.125\n",
      "Total PID Loss 0.5561756309049155 lambda= 0.125\n",
      "Total PID Loss 0.5556768400608492 lambda= 0.125\n",
      "Total PID Loss 0.5561035182098942 lambda= 0.125\n",
      "Total PID Loss 0.5558127108758504 lambda= 0.125\n",
      "Total PID Loss 0.5556985221388367 lambda= 0.125\n",
      "Total PID Loss 0.5558531894317804 lambda= 0.125\n",
      "Total PID Loss 0.5556859082343658 lambda= 0.125\n",
      "Total PID Loss 0.5559234541536123 lambda= 0.125\n",
      "Total PID Loss 0.555683648194661 lambda= 0.125\n",
      "Total PID Loss 0.5556953518072155 lambda= 0.125\n",
      "Total PID Loss 0.5556606428755516 lambda= 0.125\n",
      "Total PID Loss 0.5557887855559833 lambda= 0.125\n",
      "Total PID Loss 0.5556508710374563 lambda= 0.125\n",
      "Total PID Loss 0.5557189584059169 lambda= 0.125\n",
      "Total PID Loss 0.5556566230003603 lambda= 0.125\n",
      "Total PID Loss 0.5556790895390635 lambda= 0.125\n",
      "Total PID Loss 0.5556563196843255 lambda= 0.125\n",
      "Total PID Loss 0.5556539104382756 lambda= 0.125\n",
      "Total PID Loss 0.5556492783602415 lambda= 0.125\n",
      "Total PID Loss 0.5556606036270806 lambda= 0.125\n",
      "Total PID Loss 0.5556662498930065 lambda= 0.125\n",
      "Total PID Loss 0.5556447861703346 lambda= 0.125\n",
      "Total PID Loss 0.5556743600087966 lambda= 0.125\n",
      "Total PID Loss 0.5556479557756382 lambda= 0.125\n",
      "Total PID Loss 0.5556494305083554 lambda= 0.125\n",
      "Total PID Loss 0.5556494563504087 lambda= 0.125\n",
      "Total PID Loss 0.555649645645304 lambda= 0.125\n",
      "Total PID Loss 0.5556478167616932 lambda= 0.125\n",
      "Total PID Loss 0.5556486193049128 lambda= 0.125\n",
      "Total PID Loss 0.5556445634030411 lambda= 0.125\n",
      "Total PID Loss 0.5556467294562982 lambda= 0.125\n",
      "Total PID Loss 0.5556458735270746 lambda= 0.125\n",
      "Total PID Loss 0.5556431595166812 lambda= 0.125\n",
      "Total PID Loss 0.5556488123441964 lambda= 0.125\n",
      "Total PID Loss 0.5556488562626639 lambda= 0.125\n",
      "Total PID Loss 0.5556465382635363 lambda= 0.125\n",
      "Total PID Loss 0.5556490474554258 lambda= 0.125\n",
      "Total PID Loss 0.5556464426671552 lambda= 0.125\n",
      "Total PID Loss 0.5556491430518068 lambda= 0.125\n",
      "Total PID Loss 0.5556478406576715 lambda= 0.125\n",
      "Total PID Loss 0.5556477109339536 lambda= 0.125\n",
      "Total PID Loss 0.555643972843508 lambda= 0.125\n",
      "Total PID Loss 0.5556470614581818 lambda= 0.125\n",
      "Total PID Loss 0.5556449423428365 lambda= 0.125\n",
      "Total PID Loss 0.555648739449074 lambda= 0.125\n",
      "Total PID Loss 0.5556455643773479 lambda= 0.125\n",
      "Total PID Loss 0.5556467067036948 lambda= 0.125\n",
      "Total PID Loss 0.5556443917069307 lambda= 0.125\n",
      "Total PID Loss 0.6547702902906585 lambda= 0.25\n",
      "Total PID Loss 0.6503174374631961 lambda= 0.25\n",
      "Total PID Loss 0.6535115409299233 lambda= 0.25\n",
      "Total PID Loss 0.6573394084027715 lambda= 0.25\n",
      "Total PID Loss 0.657161774061567 lambda= 0.25\n",
      "Total PID Loss 0.6507197249614449 lambda= 0.25\n",
      "Total PID Loss 0.6483769525291487 lambda= 0.25\n",
      "Total PID Loss 0.6456143149884243 lambda= 0.25\n",
      "Total PID Loss 0.6461824642243534 lambda= 0.25\n",
      "Total PID Loss 0.6446305568453441 lambda= 0.25\n",
      "Total PID Loss 0.6434221722958187 lambda= 0.25\n",
      "Total PID Loss 0.6434911909902672 lambda= 0.25\n",
      "Total PID Loss 0.6438132855224923 lambda= 0.25\n",
      "Total PID Loss 0.6442988549511353 lambda= 0.25\n",
      "Total PID Loss 0.6475806431037385 lambda= 0.25\n",
      "Total PID Loss 0.6436287566094914 lambda= 0.25\n",
      "Total PID Loss 0.6448366301722017 lambda= 0.25\n",
      "Total PID Loss 0.6435230709898415 lambda= 0.25\n",
      "Total PID Loss 0.64448932235945 lambda= 0.25\n",
      "Total PID Loss 0.6433946202238255 lambda= 0.25\n",
      "Total PID Loss 0.6439340959193894 lambda= 0.25\n",
      "Total PID Loss 0.6433788673431824 lambda= 0.25\n",
      "Total PID Loss 0.6438770028252053 lambda= 0.25\n",
      "Total PID Loss 0.6433588496569558 lambda= 0.25\n",
      "Total PID Loss 0.6434109292143475 lambda= 0.25\n",
      "Total PID Loss 0.6434171034959146 lambda= 0.25\n",
      "Total PID Loss 0.643350680502017 lambda= 0.25\n",
      "Total PID Loss 0.6433249885083934 lambda= 0.25\n",
      "Total PID Loss 0.6434389734535226 lambda= 0.25\n",
      "Total PID Loss 0.6433578158823525 lambda= 0.25\n",
      "Total PID Loss 0.6434001114012444 lambda= 0.25\n",
      "Total PID Loss 0.6433195205889135 lambda= 0.25\n",
      "Total PID Loss 0.6433346692828671 lambda= 0.25\n",
      "Total PID Loss 0.643299717982462 lambda= 0.25\n",
      "Total PID Loss 0.6433772757035336 lambda= 0.25\n",
      "Total PID Loss 0.6435266863384954 lambda= 0.25\n",
      "Total PID Loss 0.643294566673961 lambda= 0.25\n",
      "Total PID Loss 0.6434009588365661 lambda= 0.25\n",
      "Total PID Loss 0.6432941207297065 lambda= 0.25\n",
      "Total PID Loss 0.6432666979660364 lambda= 0.25\n",
      "Total PID Loss 0.6432739616237422 lambda= 0.25\n",
      "Total PID Loss 0.6432906422717769 lambda= 0.25\n",
      "Total PID Loss 0.6432506853566811 lambda= 0.25\n",
      "Total PID Loss 0.643245272180643 lambda= 0.25\n",
      "Total PID Loss 0.6432259752162168 lambda= 0.25\n",
      "Total PID Loss 0.6432083187483859 lambda= 0.25\n",
      "Total PID Loss 0.6432396948176906 lambda= 0.25\n",
      "Total PID Loss 0.6432220083905043 lambda= 0.25\n",
      "Total PID Loss 0.6431858293363764 lambda= 0.25\n",
      "Total PID Loss 0.6431921347765726 lambda= 0.25\n",
      "Total PID Loss 0.6431441646902947 lambda= 0.25\n",
      "Total PID Loss 0.6430990171620039 lambda= 0.25\n",
      "Total PID Loss 0.6433203952430141 lambda= 0.25\n",
      "Total PID Loss 0.6431860410703597 lambda= 0.25\n",
      "Total PID Loss 0.6431222400178445 lambda= 0.25\n",
      "Total PID Loss 0.6430836973735907 lambda= 0.25\n",
      "Total PID Loss 0.6430477472907716 lambda= 0.25\n",
      "Total PID Loss 0.6430243564917906 lambda= 0.25\n",
      "Total PID Loss 0.6429585918057521 lambda= 0.25\n",
      "Total PID Loss 0.6429923031354311 lambda= 0.25\n",
      "Total PID Loss 0.6429052654447076 lambda= 0.25\n",
      "Total PID Loss 0.6428250486749154 lambda= 0.25\n",
      "Total PID Loss 0.6428446092953155 lambda= 0.25\n",
      "Total PID Loss 0.6427476514739625 lambda= 0.25\n",
      "Total PID Loss 0.6426486805273564 lambda= 0.25\n",
      "Total PID Loss 0.6426654548317894 lambda= 0.25\n",
      "Total PID Loss 0.6426890621467705 lambda= 0.25\n",
      "Total PID Loss 0.6425864839857585 lambda= 0.25\n",
      "Total PID Loss 0.642576005176255 lambda= 0.25\n",
      "Total PID Loss 0.6425505560275617 lambda= 0.25\n",
      "Total PID Loss 0.6426935894251613 lambda= 0.25\n",
      "Total PID Loss 0.6427200172771552 lambda= 0.25\n",
      "Total PID Loss 0.6426008774053027 lambda= 0.25\n",
      "Total PID Loss 0.642582952134883 lambda= 0.25\n",
      "Total PID Loss 0.6426697850314698 lambda= 0.25\n",
      "Total PID Loss 0.642572732542138 lambda= 0.25\n",
      "Total PID Loss 0.6426074659679087 lambda= 0.25\n",
      "Total PID Loss 0.6425621059964157 lambda= 0.25\n",
      "Total PID Loss 0.6426070374833803 lambda= 0.25\n",
      "Total PID Loss 0.6425548967962414 lambda= 0.25\n",
      "Total PID Loss 0.6425830498908554 lambda= 0.25\n",
      "Total PID Loss 0.6425500337294424 lambda= 0.25\n",
      "Total PID Loss 0.6425776613227057 lambda= 0.25\n",
      "Total PID Loss 0.6425526929970415 lambda= 0.25\n",
      "Total PID Loss 0.6425604711482484 lambda= 0.25\n",
      "Total PID Loss 0.6425505220811369 lambda= 0.25\n",
      "Total PID Loss 0.6425516508711357 lambda= 0.25\n",
      "Total PID Loss 0.6425734333983577 lambda= 0.25\n",
      "Total PID Loss 0.6425487994741762 lambda= 0.25\n",
      "Total PID Loss 0.642558215000774 lambda= 0.25\n",
      "Total PID Loss 0.6425504221623559 lambda= 0.25\n",
      "Total PID Loss 0.6425637032381649 lambda= 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 0.6425481993551223 lambda= 0.25\n",
      "Total PID Loss 0.6425530425437744 lambda= 0.25\n",
      "Total PID Loss 0.6425484490883546 lambda= 0.25\n",
      "Total PID Loss 0.6425481701155487 lambda= 0.25\n",
      "Total PID Loss 0.6425519757926149 lambda= 0.25\n",
      "Total PID Loss 0.6425459198839539 lambda= 0.25\n",
      "Total PID Loss 0.642550171396109 lambda= 0.25\n",
      "Total PID Loss 0.6425519759641969 lambda= 0.25\n",
      "Total PID Loss 0.6425462509102602 lambda= 0.25\n",
      "Total PID Loss 0.6425433571682769 lambda= 0.25\n",
      "Total PID Loss 0.6425460254105442 lambda= 0.25\n",
      "Total PID Loss 0.6425492915878059 lambda= 0.25\n",
      "Total PID Loss 0.6425497043511987 lambda= 0.25\n",
      "Total PID Loss 0.6425475301035287 lambda= 0.25\n",
      "Total PID Loss 0.6425456554936253 lambda= 0.25\n",
      "Total PID Loss 0.6425479450158875 lambda= 0.25\n",
      "Total PID Loss 0.642543738138643 lambda= 0.25\n",
      "Total PID Loss 0.7194834979290611 lambda= 0.375\n",
      "Total PID Loss 0.716510669814403 lambda= 0.375\n",
      "Total PID Loss 0.718259571257563 lambda= 0.375\n",
      "Total PID Loss 0.720597719716844 lambda= 0.375\n",
      "Total PID Loss 0.7220337466404658 lambda= 0.375\n",
      "Total PID Loss 0.7159630833564461 lambda= 0.375\n",
      "Total PID Loss 0.7141183669702639 lambda= 0.375\n",
      "Total PID Loss 0.7142860662394925 lambda= 0.375\n",
      "Total PID Loss 0.7133988796043507 lambda= 0.375\n",
      "Total PID Loss 0.7128361147444039 lambda= 0.375\n",
      "Total PID Loss 0.712844678729933 lambda= 0.375\n",
      "Total PID Loss 0.713530444673297 lambda= 0.375\n",
      "Total PID Loss 0.7141920232666191 lambda= 0.375\n",
      "Total PID Loss 0.7131603023683428 lambda= 0.375\n",
      "Total PID Loss 0.7165022393754533 lambda= 0.375\n",
      "Total PID Loss 0.7129058477195831 lambda= 0.375\n",
      "Total PID Loss 0.7131800700350255 lambda= 0.375\n",
      "Total PID Loss 0.712869491084774 lambda= 0.375\n",
      "Total PID Loss 0.7133870728009948 lambda= 0.375\n",
      "Total PID Loss 0.7128770838725977 lambda= 0.375\n",
      "Total PID Loss 0.713093848035409 lambda= 0.375\n",
      "Total PID Loss 0.712812995158317 lambda= 0.375\n",
      "Total PID Loss 0.7129195588363844 lambda= 0.375\n",
      "Total PID Loss 0.7128247430400902 lambda= 0.375\n",
      "Total PID Loss 0.7129397541176583 lambda= 0.375\n",
      "Total PID Loss 0.7128241678949999 lambda= 0.375\n",
      "Total PID Loss 0.712855536685423 lambda= 0.375\n",
      "Total PID Loss 0.7128172437339642 lambda= 0.375\n",
      "Total PID Loss 0.7127960476717812 lambda= 0.375\n",
      "Total PID Loss 0.7128021795831077 lambda= 0.375\n",
      "Total PID Loss 0.7128470742329391 lambda= 0.375\n",
      "Total PID Loss 0.7128060512852715 lambda= 0.375\n",
      "Total PID Loss 0.7127882192404533 lambda= 0.375\n",
      "Total PID Loss 0.7127874482117373 lambda= 0.375\n",
      "Total PID Loss 0.7127941056474867 lambda= 0.375\n",
      "Total PID Loss 0.7127833049518553 lambda= 0.375\n",
      "Total PID Loss 0.7128051080101392 lambda= 0.375\n",
      "Total PID Loss 0.7127616678478731 lambda= 0.375\n",
      "Total PID Loss 0.7127499686730595 lambda= 0.375\n",
      "Total PID Loss 0.7127552010625142 lambda= 0.375\n",
      "Total PID Loss 0.7127937887337993 lambda= 0.375\n",
      "Total PID Loss 0.7127675686361714 lambda= 0.375\n",
      "Total PID Loss 0.712734135770461 lambda= 0.375\n",
      "Total PID Loss 0.7127262275849557 lambda= 0.375\n",
      "Total PID Loss 0.712733805077807 lambda= 0.375\n",
      "Total PID Loss 0.7127767448483456 lambda= 0.375\n",
      "Total PID Loss 0.7127441846505264 lambda= 0.375\n",
      "Total PID Loss 0.7127519804184299 lambda= 0.375\n",
      "Total PID Loss 0.7127343859582302 lambda= 0.375\n",
      "Total PID Loss 0.7127218377721618 lambda= 0.375\n",
      "Total PID Loss 0.7127323733727584 lambda= 0.375\n",
      "Total PID Loss 0.712752278098302 lambda= 0.375\n",
      "Total PID Loss 0.7127303279457238 lambda= 0.375\n",
      "Total PID Loss 0.7127278684098597 lambda= 0.375\n",
      "Total PID Loss 0.7127252078812389 lambda= 0.375\n",
      "Total PID Loss 0.712722475016018 lambda= 0.375\n",
      "Total PID Loss 0.7127205662092382 lambda= 0.375\n",
      "Total PID Loss 0.7127234895276855 lambda= 0.375\n",
      "Total PID Loss 0.712730560802989 lambda= 0.375\n",
      "Total PID Loss 0.7127243063835426 lambda= 0.375\n",
      "Total PID Loss 0.7127305790959555 lambda= 0.375\n",
      "Total PID Loss 0.7127243820018583 lambda= 0.375\n",
      "Total PID Loss 0.7127227302430965 lambda= 0.375\n",
      "Total PID Loss 0.7127225733680955 lambda= 0.375\n",
      "Total PID Loss 0.7127233742432166 lambda= 0.375\n",
      "Total PID Loss 0.7127231714890818 lambda= 0.375\n",
      "Total PID Loss 0.7127193841790544 lambda= 0.375\n",
      "Total PID Loss 0.7127238989161844 lambda= 0.375\n",
      "Total PID Loss 0.7127197519770212 lambda= 0.375\n",
      "Total PID Loss 0.7127216482261673 lambda= 0.375\n",
      "Total PID Loss 0.7127251686099598 lambda= 0.375\n",
      "Total PID Loss 0.7127245672110427 lambda= 0.375\n",
      "Total PID Loss 0.7127177502663923 lambda= 0.375\n",
      "Total PID Loss 0.7127198339432281 lambda= 0.375\n",
      "Total PID Loss 0.7127185571400472 lambda= 0.375\n",
      "Total PID Loss 0.7127259789137392 lambda= 0.375\n",
      "Total PID Loss 0.7770892703847624 lambda= 0.5\n",
      "Total PID Loss 0.7746136333620746 lambda= 0.5\n",
      "Total PID Loss 0.7762548223906762 lambda= 0.5\n",
      "Total PID Loss 0.7782673244307452 lambda= 0.5\n",
      "Total PID Loss 0.7788131564318239 lambda= 0.5\n",
      "Total PID Loss 0.7746077127236182 lambda= 0.5\n",
      "Total PID Loss 0.7733714165921515 lambda= 0.5\n",
      "Total PID Loss 0.7732157182882535 lambda= 0.5\n",
      "Total PID Loss 0.7724942711180846 lambda= 0.5\n",
      "Total PID Loss 0.772485611758555 lambda= 0.5\n",
      "Total PID Loss 0.7731162813902199 lambda= 0.5\n",
      "Total PID Loss 0.772779751718854 lambda= 0.5\n",
      "Total PID Loss 0.7737927874181735 lambda= 0.5\n",
      "Total PID Loss 0.7727395734774867 lambda= 0.5\n",
      "Total PID Loss 0.7741499958426605 lambda= 0.5\n",
      "Total PID Loss 0.7726053807038733 lambda= 0.5\n",
      "Total PID Loss 0.7728796104029934 lambda= 0.5\n",
      "Total PID Loss 0.7725493841272653 lambda= 0.5\n",
      "Total PID Loss 0.7729170300966675 lambda= 0.5\n",
      "Total PID Loss 0.7725274503416146 lambda= 0.5\n",
      "Total PID Loss 0.7726930675416326 lambda= 0.5\n",
      "Total PID Loss 0.7724915135810622 lambda= 0.5\n",
      "Total PID Loss 0.7726193383649038 lambda= 0.5\n",
      "Total PID Loss 0.7724866226713885 lambda= 0.5\n",
      "Total PID Loss 0.7725891694124327 lambda= 0.5\n",
      "Total PID Loss 0.7724843452470426 lambda= 0.5\n",
      "Total PID Loss 0.7724635301090612 lambda= 0.5\n",
      "Total PID Loss 0.7724657577284261 lambda= 0.5\n",
      "Total PID Loss 0.7725062223055936 lambda= 0.5\n",
      "Total PID Loss 0.7724725733911695 lambda= 0.5\n",
      "Total PID Loss 0.7724782902354543 lambda= 0.5\n",
      "Total PID Loss 0.7724756403867378 lambda= 0.5\n",
      "Total PID Loss 0.7724653170639546 lambda= 0.5\n",
      "Total PID Loss 0.7724894723584159 lambda= 0.5\n",
      "Total PID Loss 0.7724663990815315 lambda= 0.5\n",
      "Total PID Loss 0.7724849432426113 lambda= 0.5\n",
      "Total PID Loss 0.7724668105714354 lambda= 0.5\n",
      "Total PID Loss 0.7724540922738723 lambda= 0.5\n",
      "Total PID Loss 0.7724445692133872 lambda= 0.5\n",
      "Total PID Loss 0.7724417059462358 lambda= 0.5\n",
      "Total PID Loss 0.7724451165336784 lambda= 0.5\n",
      "Total PID Loss 0.7724437542391456 lambda= 0.5\n",
      "Total PID Loss 0.7724636430306022 lambda= 0.5\n",
      "Total PID Loss 0.7724500651959154 lambda= 0.5\n",
      "Total PID Loss 0.7724308799362305 lambda= 0.5\n",
      "Total PID Loss 0.7724134507872138 lambda= 0.5\n",
      "Total PID Loss 0.7724261788959436 lambda= 0.5\n",
      "Total PID Loss 0.7724210593128726 lambda= 0.5\n",
      "Total PID Loss 0.772436743035275 lambda= 0.5\n",
      "Total PID Loss 0.7724000154435098 lambda= 0.5\n",
      "Total PID Loss 0.7723941702805053 lambda= 0.5\n",
      "Total PID Loss 0.7724092475385527 lambda= 0.5\n",
      "Total PID Loss 0.7723916133161484 lambda= 0.5\n",
      "Total PID Loss 0.7723904685196503 lambda= 0.5\n",
      "Total PID Loss 0.7723883247200364 lambda= 0.5\n",
      "Total PID Loss 0.7724056596984552 lambda= 0.5\n",
      "Total PID Loss 0.7723844884571305 lambda= 0.5\n",
      "Total PID Loss 0.7724174923030438 lambda= 0.5\n",
      "Total PID Loss 0.7724086558367584 lambda= 0.5\n",
      "Total PID Loss 0.7723938941611073 lambda= 0.5\n",
      "Total PID Loss 0.7724219235487314 lambda= 0.5\n",
      "Total PID Loss 0.7723879637088937 lambda= 0.5\n",
      "Total PID Loss 0.7723865827273309 lambda= 0.5\n",
      "Total PID Loss 0.772381440693453 lambda= 0.5\n",
      "Total PID Loss 0.7723754725310379 lambda= 0.5\n",
      "Total PID Loss 0.7723859913029456 lambda= 0.5\n",
      "Total PID Loss 0.7723808621122468 lambda= 0.5\n",
      "Total PID Loss 0.7723875294996376 lambda= 0.5\n",
      "Total PID Loss 0.7723780063778298 lambda= 0.5\n",
      "Total PID Loss 0.7723907390582172 lambda= 0.5\n",
      "Total PID Loss 0.7723856702882078 lambda= 0.5\n",
      "Total PID Loss 0.772382402261935 lambda= 0.5\n",
      "Total PID Loss 0.7723887081793956 lambda= 0.5\n",
      "Total PID Loss 0.7723824060914668 lambda= 0.5\n",
      "Total PID Loss 0.772378874048691 lambda= 0.5\n",
      "Total PID Loss 0.772374487774804 lambda= 0.5\n",
      "Total PID Loss 0.7723820968408917 lambda= 0.5\n",
      "Total PID Loss 0.7723825629057925 lambda= 0.5\n",
      "Total PID Loss 0.7723775979831026 lambda= 0.5\n",
      "Total PID Loss 0.7723870700945842 lambda= 0.5\n",
      "Total PID Loss 0.7723845954664752 lambda= 0.5\n",
      "Total PID Loss 0.7723748389020029 lambda= 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 0.7723818260337799 lambda= 0.5\n",
      "Total PID Loss 0.772380575981827 lambda= 0.5\n",
      "Total PID Loss 0.7723766809117476 lambda= 0.5\n",
      "Total PID Loss 0.7723785619046357 lambda= 0.5\n",
      "Total PID Loss 0.8278291459173941 lambda= 0.625\n",
      "Total PID Loss 0.8258583344570792 lambda= 0.625\n",
      "Total PID Loss 0.8271935117020921 lambda= 0.625\n",
      "Total PID Loss 0.8293832712618749 lambda= 0.625\n",
      "Total PID Loss 0.8286396497780701 lambda= 0.625\n",
      "Total PID Loss 0.8257817451235527 lambda= 0.625\n",
      "Total PID Loss 0.8249156163191111 lambda= 0.625\n",
      "Total PID Loss 0.8249019765639805 lambda= 0.625\n",
      "Total PID Loss 0.8246405879398373 lambda= 0.625\n",
      "Total PID Loss 0.8246340812889359 lambda= 0.625\n",
      "Total PID Loss 0.825847067918337 lambda= 0.625\n",
      "Total PID Loss 0.8253923898689373 lambda= 0.625\n",
      "Total PID Loss 0.8268344313124031 lambda= 0.625\n",
      "Total PID Loss 0.8248068558146597 lambda= 0.625\n",
      "Total PID Loss 0.8262071584290671 lambda= 0.625\n",
      "Total PID Loss 0.8247494286180101 lambda= 0.625\n",
      "Total PID Loss 0.8249910854079243 lambda= 0.625\n",
      "Total PID Loss 0.8246711376495232 lambda= 0.625\n",
      "Total PID Loss 0.8251564108782948 lambda= 0.625\n",
      "Total PID Loss 0.8246356727298616 lambda= 0.625\n",
      "Total PID Loss 0.8247609866230535 lambda= 0.625\n",
      "Total PID Loss 0.8246352514572453 lambda= 0.625\n",
      "Total PID Loss 0.8247733120388107 lambda= 0.625\n",
      "Total PID Loss 0.8246153569827128 lambda= 0.625\n",
      "Total PID Loss 0.8246089545797326 lambda= 0.625\n",
      "Total PID Loss 0.8246414428581075 lambda= 0.625\n",
      "Total PID Loss 0.8246852358013627 lambda= 0.625\n",
      "Total PID Loss 0.824598874330617 lambda= 0.625\n",
      "Total PID Loss 0.8246400492888972 lambda= 0.625\n",
      "Total PID Loss 0.8246146827426087 lambda= 0.625\n",
      "Total PID Loss 0.8246693170467638 lambda= 0.625\n",
      "Total PID Loss 0.8246147826394108 lambda= 0.625\n",
      "Total PID Loss 0.8245976954776166 lambda= 0.625\n",
      "Total PID Loss 0.8246137002530882 lambda= 0.625\n",
      "Total PID Loss 0.824596552863782 lambda= 0.625\n",
      "Total PID Loss 0.8246199083128991 lambda= 0.625\n",
      "Total PID Loss 0.8246184239004242 lambda= 0.625\n",
      "Total PID Loss 0.8245928297979845 lambda= 0.625\n",
      "Total PID Loss 0.8246094042995837 lambda= 0.625\n",
      "Total PID Loss 0.8245976076254428 lambda= 0.625\n",
      "Total PID Loss 0.8245923778648088 lambda= 0.625\n",
      "Total PID Loss 0.8246014061449963 lambda= 0.625\n",
      "Total PID Loss 0.8246114940550174 lambda= 0.625\n",
      "Total PID Loss 0.8245985507117284 lambda= 0.625\n",
      "Total PID Loss 0.8245999740258478 lambda= 0.625\n",
      "Total PID Loss 0.8245872364207621 lambda= 0.625\n",
      "Total PID Loss 0.8245876225506743 lambda= 0.625\n",
      "Total PID Loss 0.8246011749277586 lambda= 0.625\n",
      "Total PID Loss 0.8245884275089157 lambda= 0.625\n",
      "Total PID Loss 0.8245853695921018 lambda= 0.625\n",
      "Total PID Loss 0.8245947066362701 lambda= 0.625\n",
      "Total PID Loss 0.8738578723306112 lambda= 0.75\n",
      "Total PID Loss 0.8720954319689589 lambda= 0.75\n",
      "Total PID Loss 0.8733104494988395 lambda= 0.75\n",
      "Total PID Loss 0.875104639959567 lambda= 0.75\n",
      "Total PID Loss 0.8746271192955186 lambda= 0.75\n",
      "Total PID Loss 0.872159002649599 lambda= 0.75\n",
      "Total PID Loss 0.8716523929687403 lambda= 0.75\n",
      "Total PID Loss 0.8712248625360715 lambda= 0.75\n",
      "Total PID Loss 0.8711846397957356 lambda= 0.75\n",
      "Total PID Loss 0.8714293626238349 lambda= 0.75\n",
      "Total PID Loss 0.8712731624926505 lambda= 0.75\n",
      "Total PID Loss 0.8713475899338972 lambda= 0.75\n",
      "Total PID Loss 0.8726672448780688 lambda= 0.75\n",
      "Total PID Loss 0.871323382575028 lambda= 0.75\n",
      "Total PID Loss 0.871691674638405 lambda= 0.75\n",
      "Total PID Loss 0.8711880911235502 lambda= 0.75\n",
      "Total PID Loss 0.871502950424017 lambda= 0.75\n",
      "Total PID Loss 0.8711860916249508 lambda= 0.75\n",
      "Total PID Loss 0.8713639738158299 lambda= 0.75\n",
      "Total PID Loss 0.8711908504055982 lambda= 0.75\n",
      "Total PID Loss 0.8711075800105494 lambda= 0.75\n",
      "Total PID Loss 0.8710858886679143 lambda= 0.75\n",
      "Total PID Loss 0.8710949790331057 lambda= 0.75\n",
      "Total PID Loss 0.8711785053057395 lambda= 0.75\n",
      "Total PID Loss 0.8710407138766851 lambda= 0.75\n",
      "Total PID Loss 0.8709960176886022 lambda= 0.75\n",
      "Total PID Loss 0.8709761298402199 lambda= 0.75\n",
      "Total PID Loss 0.8709364027394069 lambda= 0.75\n",
      "Total PID Loss 0.8711511713948557 lambda= 0.75\n",
      "Total PID Loss 0.8710482144940849 lambda= 0.75\n",
      "Total PID Loss 0.8710986851297781 lambda= 0.75\n",
      "Total PID Loss 0.8710301199573512 lambda= 0.75\n",
      "Total PID Loss 0.8709087133228441 lambda= 0.75\n",
      "Total PID Loss 0.8708843423418771 lambda= 0.75\n",
      "Total PID Loss 0.8709171970450764 lambda= 0.75\n",
      "Total PID Loss 0.8708782831690474 lambda= 0.75\n",
      "Total PID Loss 0.8708850699618124 lambda= 0.75\n",
      "Total PID Loss 0.8708663837072145 lambda= 0.75\n",
      "Total PID Loss 0.8709235853821068 lambda= 0.75\n",
      "Total PID Loss 0.8708628461798216 lambda= 0.75\n",
      "Total PID Loss 0.8709438580615951 lambda= 0.75\n",
      "Total PID Loss 0.8709103345170168 lambda= 0.75\n",
      "Total PID Loss 0.8708684555879309 lambda= 0.75\n",
      "Total PID Loss 0.8708844219774776 lambda= 0.75\n",
      "Total PID Loss 0.8708604387707816 lambda= 0.75\n",
      "Total PID Loss 0.8708657451997464 lambda= 0.75\n",
      "Total PID Loss 0.8708532423290077 lambda= 0.75\n",
      "Total PID Loss 0.870882161866918 lambda= 0.75\n",
      "Total PID Loss 0.8708893102105058 lambda= 0.75\n",
      "Total PID Loss 0.870855255735009 lambda= 0.75\n",
      "Total PID Loss 0.8708582650596552 lambda= 0.75\n",
      "Total PID Loss 0.870872992259907 lambda= 0.75\n",
      "Total PID Loss 0.8708541227026739 lambda= 0.75\n",
      "Total PID Loss 0.8708820373509543 lambda= 0.75\n",
      "Total PID Loss 0.8708533868093755 lambda= 0.75\n",
      "Total PID Loss 0.8708638664923125 lambda= 0.75\n",
      "Total PID Loss 0.8708556766672328 lambda= 0.75\n",
      "Total PID Loss 0.870851064886061 lambda= 0.75\n",
      "Total PID Loss 0.8708638664923125 lambda= 0.75\n",
      "Total PID Loss 0.8708752733396125 lambda= 0.75\n",
      "Total PID Loss 0.8708559227700399 lambda= 0.75\n",
      "Total PID Loss 0.8708436201262126 lambda= 0.75\n",
      "Total PID Loss 0.8708520845968 lambda= 0.75\n",
      "Total PID Loss 0.8708685397731897 lambda= 0.75\n",
      "Total PID Loss 0.8708446268292129 lambda= 0.75\n",
      "Total PID Loss 0.8708677251358383 lambda= 0.75\n",
      "Total PID Loss 0.870855689065104 lambda= 0.75\n",
      "Total PID Loss 0.8708632263794441 lambda= 0.75\n",
      "Total PID Loss 0.8708568010254013 lambda= 0.75\n",
      "Total PID Loss 0.8708526569590347 lambda= 0.75\n",
      "Total PID Loss 0.8708558759874585 lambda= 0.75\n",
      "Total PID Loss 0.8708565270937461 lambda= 0.75\n",
      "Total PID Loss 0.8708495133447403 lambda= 0.75\n",
      "Total PID Loss 0.8708443061149768 lambda= 0.75\n",
      "Total PID Loss 0.8708565217495033 lambda= 0.75\n",
      "Total PID Loss 0.8708675754152133 lambda= 0.75\n",
      "Total PID Loss 0.8708438218696765 lambda= 0.75\n",
      "Total PID Loss 0.8708536967311408 lambda= 0.75\n",
      "Total PID Loss 0.8708554097892061 lambda= 0.75\n",
      "Total PID Loss 0.8708496068059174 lambda= 0.75\n",
      "Total PID Loss 0.8708560502070082 lambda= 0.75\n",
      "Total PID Loss 0.8708590642660297 lambda= 0.75\n",
      "Total PID Loss 0.8708523957301846 lambda= 0.75\n",
      "Total PID Loss 0.8708624181939694 lambda= 0.75\n",
      "Total PID Loss 0.8708573331609166 lambda= 0.75\n",
      "Total PID Loss 0.8708567862042919 lambda= 0.75\n",
      "Total PID Loss 0.9145996722964738 lambda= 0.875\n",
      "Total PID Loss 0.9130519037882615 lambda= 0.875\n",
      "Total PID Loss 0.9141918277217738 lambda= 0.875\n",
      "Total PID Loss 0.9155409439961186 lambda= 0.875\n",
      "Total PID Loss 0.9154167484774168 lambda= 0.875\n",
      "Total PID Loss 0.9132439863236606 lambda= 0.875\n",
      "Total PID Loss 0.9127331774766138 lambda= 0.875\n",
      "Total PID Loss 0.912477138025017 lambda= 0.875\n",
      "Total PID Loss 0.9124179268481938 lambda= 0.875\n",
      "Total PID Loss 0.9127428582640388 lambda= 0.875\n",
      "Total PID Loss 0.9125684858497245 lambda= 0.875\n",
      "Total PID Loss 0.9127896184364377 lambda= 0.875\n",
      "Total PID Loss 0.9141835091510814 lambda= 0.875\n",
      "Total PID Loss 0.9124504870649559 lambda= 0.875\n",
      "Total PID Loss 0.912837387216242 lambda= 0.875\n",
      "Total PID Loss 0.9125141698625949 lambda= 0.875\n",
      "Total PID Loss 0.912586488933671 lambda= 0.875\n",
      "Total PID Loss 0.9124533397307077 lambda= 0.875\n",
      "Total PID Loss 0.9125763926687259 lambda= 0.875\n",
      "Total PID Loss 0.912435662478327 lambda= 0.875\n",
      "Total PID Loss 0.9123235352372031 lambda= 0.875\n",
      "Total PID Loss 0.9122926266557279 lambda= 0.875\n",
      "Total PID Loss 0.9124307415911036 lambda= 0.875\n",
      "Total PID Loss 0.912333852251338 lambda= 0.875\n",
      "Total PID Loss 0.9123681863000976 lambda= 0.875\n",
      "Total PID Loss 0.9123265006997261 lambda= 0.875\n",
      "Total PID Loss 0.9122063746484284 lambda= 0.875\n",
      "Total PID Loss 0.9121575205440144 lambda= 0.875\n",
      "Total PID Loss 0.9124520146440975 lambda= 0.875\n",
      "Total PID Loss 0.9122831820148729 lambda= 0.875\n",
      "Total PID Loss 0.9121793160363126 lambda= 0.875\n",
      "Total PID Loss 0.9122077101554454 lambda= 0.875\n",
      "Total PID Loss 0.9121616343907981 lambda= 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 0.9121326650252806 lambda= 0.875\n",
      "Total PID Loss 0.9121772262537396 lambda= 0.875\n",
      "Total PID Loss 0.9121647239998911 lambda= 0.875\n",
      "Total PID Loss 0.9121510944040901 lambda= 0.875\n",
      "Total PID Loss 0.9121460061976384 lambda= 0.875\n",
      "Total PID Loss 0.9121445547372701 lambda= 0.875\n",
      "Total PID Loss 0.9122043718356306 lambda= 0.875\n",
      "Total PID Loss 0.9121443400261395 lambda= 0.875\n",
      "Total PID Loss 0.9121396583960656 lambda= 0.875\n",
      "Total PID Loss 0.912144582164768 lambda= 0.875\n",
      "Total PID Loss 0.9121364235873075 lambda= 0.875\n",
      "Total PID Loss 0.9121800857689133 lambda= 0.875\n",
      "Total PID Loss 0.9121275711891419 lambda= 0.875\n",
      "Total PID Loss 0.9121445639617424 lambda= 0.875\n",
      "Total PID Loss 0.9121373766870031 lambda= 0.875\n",
      "Total PID Loss 0.912155135700858 lambda= 0.875\n",
      "Total PID Loss 0.9121297810469033 lambda= 0.875\n",
      "Total PID Loss 0.9121358230073419 lambda= 0.875\n",
      "Total PID Loss 0.9121467375889196 lambda= 0.875\n",
      "Total PID Loss 0.9121249634416364 lambda= 0.875\n",
      "Total PID Loss 0.912131787865086 lambda= 0.875\n",
      "Total PID Loss 0.9121284461221486 lambda= 0.875\n",
      "Total PID Loss 0.9121169668862558 lambda= 0.875\n",
      "Total PID Loss 0.9121159000435779 lambda= 0.875\n",
      "Total PID Loss 0.9121329804992596 lambda= 0.875\n",
      "Total PID Loss 0.9121269853338695 lambda= 0.875\n",
      "Total PID Loss 0.9121233232580098 lambda= 0.875\n",
      "Total PID Loss 0.9121246409979783 lambda= 0.875\n",
      "Total PID Loss 0.912131467182806 lambda= 0.875\n",
      "Total PID Loss 0.9121244701728123 lambda= 0.875\n",
      "Total PID Loss 0.9121192037945529 lambda= 0.875\n",
      "Total PID Loss 0.9121332718041819 lambda= 0.875\n",
      "Total PID Loss 0.912113576980673 lambda= 0.875\n",
      "Total PID Loss 0.9121245016609053 lambda= 0.875\n",
      "Total PID Loss 0.9121196697079077 lambda= 0.875\n",
      "Total PID Loss 0.9121211137772107 lambda= 0.875\n",
      "Total PID Loss 0.9121284735940705 lambda= 0.875\n",
      "Total PID Loss 0.9121145972612075 lambda= 0.875\n",
      "Total PID Loss 0.9121127547643217 lambda= 0.875\n",
      "Total PID Loss 0.9121231082419456 lambda= 0.875\n",
      "Total PID Loss 0.912114673245132 lambda= 0.875\n",
      "Total PID Loss 0.9121185902788775 lambda= 0.875\n",
      "Total PID Loss 0.9121247800301179 lambda= 0.875\n",
      "Total PID Loss 0.9121267807658171 lambda= 0.875\n",
      "Total PID Loss 0.9121274321570024 lambda= 0.875\n",
      "Total PID Loss 0.9121274701489648 lambda= 0.875\n",
      "Total PID Loss 0.9121279107731621 lambda= 0.875\n",
      "Total PID Loss 0.9510978587811584 lambda= 1.0\n",
      "Total PID Loss 0.9496831392904251 lambda= 1.0\n",
      "Total PID Loss 0.9508050773128565 lambda= 1.0\n",
      "Total PID Loss 0.951786665013066 lambda= 1.0\n",
      "Total PID Loss 0.951911683201462 lambda= 1.0\n",
      "Total PID Loss 0.9499271677865552 lambda= 1.0\n",
      "Total PID Loss 0.9495226131848944 lambda= 1.0\n",
      "Total PID Loss 0.9493919449597732 lambda= 1.0\n",
      "Total PID Loss 0.949277922091291 lambda= 1.0\n",
      "Total PID Loss 0.9496789342941483 lambda= 1.0\n",
      "Total PID Loss 0.9495267959294711 lambda= 1.0\n",
      "Total PID Loss 0.9498050499586224 lambda= 1.0\n",
      "Total PID Loss 0.949415935364155 lambda= 1.0\n",
      "Total PID Loss 0.950762517129382 lambda= 1.0\n",
      "Total PID Loss 0.9492810621427568 lambda= 1.0\n",
      "Total PID Loss 0.9494521503821639 lambda= 1.0\n",
      "Total PID Loss 0.9493173422232997 lambda= 1.0\n",
      "Total PID Loss 0.9495843834932347 lambda= 1.0\n",
      "Total PID Loss 0.9493219273152138 lambda= 1.0\n",
      "Total PID Loss 0.9492191719889151 lambda= 1.0\n",
      "Total PID Loss 0.9492382947995466 lambda= 1.0\n",
      "Total PID Loss 0.9494137078807319 lambda= 1.0\n",
      "Total PID Loss 0.9492683495601251 lambda= 1.0\n",
      "Total PID Loss 0.949194344358591 lambda= 1.0\n",
      "Total PID Loss 0.9491332691789912 lambda= 1.0\n",
      "Total PID Loss 0.9491808493329845 lambda= 1.0\n",
      "Total PID Loss 0.9491110491277275 lambda= 1.0\n",
      "Total PID Loss 0.9490511690180067 lambda= 1.0\n",
      "Total PID Loss 0.9490123138896706 lambda= 1.0\n",
      "Total PID Loss 0.9489319738022375 lambda= 1.0\n",
      "Total PID Loss 0.9489434262524563 lambda= 1.0\n",
      "Total PID Loss 0.9488737561722673 lambda= 1.0\n",
      "Total PID Loss 0.9488508290201295 lambda= 1.0\n",
      "Total PID Loss 0.9487809637523384 lambda= 1.0\n",
      "Total PID Loss 0.9487498217952487 lambda= 1.0\n",
      "Total PID Loss 0.9486896814353919 lambda= 1.0\n",
      "Total PID Loss 0.9486694697356821 lambda= 1.0\n",
      "Total PID Loss 0.9489995414463706 lambda= 1.0\n",
      "Total PID Loss 0.94878465852791 lambda= 1.0\n",
      "Total PID Loss 0.9487074225755421 lambda= 1.0\n",
      "Total PID Loss 0.9486450650118289 lambda= 1.0\n",
      "Total PID Loss 0.9487310740025537 lambda= 1.0\n",
      "Total PID Loss 0.9489677129475402 lambda= 1.0\n",
      "Total PID Loss 0.9486834338637271 lambda= 1.0\n",
      "Total PID Loss 0.9486295376904014 lambda= 1.0\n",
      "Total PID Loss 0.9486940987932704 lambda= 1.0\n",
      "Total PID Loss 0.9486036468078332 lambda= 1.0\n",
      "Total PID Loss 0.9485874466093651 lambda= 1.0\n",
      "Total PID Loss 0.9487081588225493 lambda= 1.0\n",
      "Total PID Loss 0.9486273341512453 lambda= 1.0\n",
      "Total PID Loss 0.9485752219957382 lambda= 1.0\n",
      "Total PID Loss 0.9486001797511302 lambda= 1.0\n",
      "Total PID Loss 0.9485639989569554 lambda= 1.0\n",
      "Total PID Loss 0.948582568736048 lambda= 1.0\n",
      "Total PID Loss 0.9486442311710208 lambda= 1.0\n",
      "Total PID Loss 0.9485851095600074 lambda= 1.0\n",
      "Total PID Loss 0.9485776446667633 lambda= 1.0\n",
      "Total PID Loss 0.9485870973524281 lambda= 1.0\n",
      "Total PID Loss 0.9485866196629896 lambda= 1.0\n",
      "Total PID Loss 0.9485636616721519 lambda= 1.0\n",
      "Total PID Loss 0.9485874466093651 lambda= 1.0\n",
      "Total PID Loss 0.9486019843280825 lambda= 1.0\n",
      "Total PID Loss 0.9485714844093329 lambda= 1.0\n",
      "Total PID Loss 0.9486197716642003 lambda= 1.0\n",
      "Total PID Loss 0.9485814045048692 lambda= 1.0\n",
      "Total PID Loss 0.948575396624207 lambda= 1.0\n",
      "Total PID Loss 0.9485674317898243 lambda= 1.0\n",
      "Total PID Loss 0.9485694418339451 lambda= 1.0\n",
      "Total PID Loss 0.9485819369772747 lambda= 1.0\n",
      "Total PID Loss 0.9485914341663397 lambda= 1.0\n",
      "Total PID Loss 0.948560684277021 lambda= 1.0\n",
      "Total PID Loss 0.9485904770948964 lambda= 1.0\n",
      "Total PID Loss 0.9485788842400429 lambda= 1.0\n",
      "Total PID Loss 0.9485623142255045 lambda= 1.0\n",
      "Total PID Loss 0.9485756243430759 lambda= 1.0\n",
      "Total PID Loss 0.9485652043064011 lambda= 1.0\n",
      "Total PID Loss 0.9485678991996958 lambda= 1.0\n",
      "Total PID Loss 0.9485755592805419 lambda= 1.0\n",
      "Total PID Loss 0.9485640092365221 lambda= 1.0\n",
      "Total PID Loss 0.948581339442335 lambda= 1.0\n",
      "Total PID Loss 0.9485769717897232 lambda= 1.0\n",
      "Total PID Loss 0.9485614992512628 lambda= 1.0\n",
      "Total PID Loss 0.9485623467567716 lambda= 1.0\n",
      "Total PID Loss 0.9485629442917111 lambda= 1.0\n",
      "Total PID Loss 0.9485642917383583 lambda= 1.0\n",
      "Total PID Loss 0.9485827194202494 lambda= 1.0\n",
      "Total PID Loss 0.9485803943430253 lambda= 1.0\n",
      "Total PID Loss 0.9485727993247133 lambda= 1.0\n",
      "Total PID Loss 0.9485730818265495 lambda= 1.0\n",
      "Total PID Loss 0.948561814284366 lambda= 1.0\n",
      "Total PID Loss 0.9485626292586079 lambda= 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m44results = createFrontier(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
