{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Frontiers for Single/Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook.\n",
    "\n",
    "# These lines import the Numpy, Pandas, Seaborn, Matplotlib modules.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing plotting libraries and styles\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# For Pandas to ignore FutureWarning displays\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell given below sets up MATLAB for the notebook\n",
    "Source: https://sehyoun.com/blog/20180904_using-matlab-with-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "import io\n",
    "import scipy.io\n",
    "from IPython.core.magic import register_cell_magic\n",
    "ip = get_ipython()\n",
    "\n",
    "out = io.StringIO()\n",
    "err = io.StringIO()\n",
    "\n",
    "# Setup matlab cell magic #\n",
    "@register_cell_magic\n",
    "def matlab_magic(line,cell):\n",
    "    out.truncate(0)\n",
    "    out.seek(0)\n",
    "    err.truncate(0)\n",
    "    err.truncate(0)\n",
    "    raw = '''{line}.eval(\"\"\"{cell}\"\"\", nargout=0, stdout=out, stderr=err)'''\n",
    "    ip.run_cell(raw.format(line=line, cell=cell))\n",
    "    print(out.getvalue())\n",
    "    print(err.getvalue())\n",
    "    \n",
    "# Starting a MATLAB engine called eng\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Change this to the file path on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adds the MMB.m as well as MMBOPT1.m and MMBOPT2.m folders to the MATLAB engine path\"\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2', nargout=0)\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/mmb-gui-mlab-2.3.2/MMB_OPTIONS', nargout=0)\n",
    "eng.addpath(r'/Users/Desktop/monetaryPolicy/scripts', nargout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important:\n",
    "The code below sets the coefficients and other data for the PID rule to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the coefficients table here:\n",
    "\n",
    "https://github.com/rishab231/monetaryPolicy/blob/master/coefficients.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This sets the coefficients of the monetary policy rule, there are 33 coefficients and len(coefficients) = 33\n",
    "coefficients = [0, 0, 0, 0, 1.5/4, 1.5/4, 1.5/4, 1.5/4, \n",
    "                0, 0, 0, 0, 0, 0.5, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "\n",
    "# Number of the model you want to chooose, please exclude 69-79, 19-22, 27, 59, 65, 68, 81, 97, 98\n",
    "modelNum = 1\n",
    "\n",
    "scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run for MMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng.MMB(nargout = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to get model name as well as the unconditional variances\n",
    "\n",
    "def getModelName():\n",
    "    irf_4 = pd.read_excel(\"../mmb-gui-mlab-2.3.2/OUTPUT/results.xls\", sheetname = \"IRF Mon. Pol. Shock      \")\n",
    "    irf_4 = irf_4.T\n",
    "    irf_headers = irf_4.iloc[0] # grab the first row for the header\n",
    "    irf_4 = irf_4[1:] # take the data less the header row\n",
    "    irf_4_stripped_headers = [myHeader.strip() for myHeader in np.array(irf_headers)] # removing trailing whitespaces\n",
    "    irf_4.columns = irf_4_stripped_headers\n",
    "    modelName = irf_4.columns.values[1]\n",
    "    return modelName\n",
    "\n",
    "def unconditionalVariances():\n",
    "    var4 = pd.read_csv(\"../mmb-gui-mlab-2.3.2/OUTPUT/variances.csv\", names=[\"interest\", \"inflation\", \"outputgap\", \"output\"])\n",
    "    return var4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NK_RW97'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getModelName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>inflation</th>\n",
       "      <th>outputgap</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079687</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.38901</td>\n",
       "      <td>1.1702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   interest  inflation  outputgap  output\n",
       "0  0.079687    0.06191    0.38901  1.1702"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unconditionalVariances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PID Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PID_loss_array = []\n",
    "PID_final_inflation_variance = 0\n",
    "PID_final_output_gap_variance = 0\n",
    "PID_final_interest_rate_variance = 0\n",
    "currentPIDLoss = 0\n",
    "\n",
    "def myPID(selected_coeff, lambdaVal, coeffInterest, modelNum, VarTarget):\n",
    "    global PID_final_inflation_variance\n",
    "    global PID_final_output_gap_variance\n",
    "    global PID_final_interest_rate_variance\n",
    "    global currentPIDLoss\n",
    "\n",
    "    coefficients = [1, 0, 0, 0, \n",
    "                    abs(selected_coeff[0])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    selected_coeff[1]/4, \n",
    "                    0, 0, 0, 0, \n",
    "                    selected_coeff[2], selected_coeff[3], 0, \n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "    scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNum)) # Input for MMB.\n",
    "    eng.MMB(nargout = 0) # Run MMB\n",
    "    \n",
    "    variances = unconditionalVariances()\n",
    "    \n",
    "    interest_rate_variance = variances['interest'][0]\n",
    "    inflation_variance = variances['inflation'][0]\n",
    "    output_gap_variance = variances['outputgap'][0]\n",
    "    \n",
    "    PID_final_inflation_variance = inflation_variance\n",
    "    PID_final_output_gap_variance = output_gap_variance\n",
    "    PID_final_interest_rate_variance = interest_rate_variance\n",
    "    \n",
    "    PID_loss = ((interest_rate_variance - VarTarget[0])**2 * coeffInterest \n",
    "                + (inflation_variance / VarTarget[1]) * lambdaVal\n",
    "                + (output_gap_variance/ VarTarget[2]) * (1 - lambdaVal) )\n",
    "    \n",
    "    print(\"myPID Loss\", PID_loss, \"lambda =\", lambdaVal)\n",
    "    PID_loss_array.append(PID_loss)\n",
    "    currentPIDLoss = PID_loss\n",
    "    \n",
    "    return PID_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frontier function which optimizes myPID as objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFrontier(modelNum):\n",
    "    taylor_coeff = [1.5, 0, 0.5, 0] # Taylor rule coefficients.\n",
    "    hamilton_coeff = [1.42, -1.20, 0.5, -0.48] # Hamilton fit coefficients.\n",
    "    weights = np.arange(0, 1.01, .25) # Values for lambda\n",
    "    interest_coeff = 1.0 # Coefficient on the interest_rate loss term\n",
    "    \n",
    "    # ---\n",
    "    rVarTarget = [16.0, 1.0, 1.0] # # Normalizing coefficients for PID_loss (initial pass)\n",
    "    mylambdatemp = 0.0 # Something to put in the lambda variable when calculating the target rate variance.\n",
    "    # Run with Hamilton coefficients to generate target rate variance.\n",
    "    myfoo = myPID(hamilton_coeff, mylambdatemp, interest_coeff, modelNum, rVarTarget)\n",
    "    VarTarget = [unconditionalVariances()['interest'][0], 1, 1] # Normalizing variances for interest rate\n",
    "    print(\"The target rate variances are\", VarTarget[0]) # Print assigned rate variance.\n",
    "    # ---\n",
    "    \n",
    "    PID_inflation_variances = []\n",
    "    PID_output_gap_variances = []\n",
    "    PID_interest_variances = []\n",
    "    PID_loss_of_lambda = []\n",
    "    PID_coefficients_array = []\n",
    "\n",
    "    # Starting with the Taylor rule coefficients\n",
    "    #current_coeff = taylor_coeff\n",
    "    \n",
    "    # Starting with the Hamilton rule coefficients\n",
    "    current_coeff = hamilton_coeff\n",
    "\n",
    "    for lambda_value in weights:\n",
    "        PID_result = scipy.optimize.minimize(myPID, current_coeff, \\\n",
    "                                    args=(lambda_value, interest_coeff, modelNum, rVarTarget), method='Nelder-Mead')\n",
    "        current_coeff = PID_result.x\n",
    "        PID_coefficients_array.append(current_coeff)\n",
    "        PID_inflation_variances.append(PID_final_inflation_variance)\n",
    "        PID_output_gap_variances.append(PID_final_output_gap_variance)\n",
    "        PID_interest_variances.append(PID_final_interest_rate_variance)\n",
    "        PID_loss_of_lambda.append(currentPIDLoss)\n",
    "    \n",
    "    # The code below plots the frontier\n",
    "    nameOfModel = getModelName()\n",
    "    \n",
    "    SD_inflation_scatter = np.sqrt(np.asarray(PID_inflation_variances))\n",
    "    SD_output_gap_scatter = np.sqrt(np.asarray(PID_output_gap_variances))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(SD_inflation_scatter, SD_output_gap_scatter, color=\"red\")\n",
    "\n",
    "    for i in range(0, len(weights)):\n",
    "        ax.annotate(weights[i], (SD_inflation_scatter[i], SD_output_gap_scatter[i]))\n",
    "\n",
    "    ax.set_xlabel('$\\sigma_{\\pi}$', fontsize=10)\n",
    "    ax.set_ylabel('$\\sigma_{y}$', fontsize=10)\n",
    "    ax.set_title('Policy Frontiers for Different Weights, Model: ' + nameOfModel, fontsize=14)\n",
    "    saveFileName = nameOfModel + '.pdf'\n",
    "    plt.savefig(saveFileName, bbox_inches='tight')\n",
    "    \n",
    "    # The code below saves the results of the all the optimizations with the appropriate lambdas into a DataFrame\n",
    "    results = dict()\n",
    "    results['lambdas'] = weights\n",
    "    results['inflation_variance'] = PID_inflation_variances\n",
    "    results['output_gap_variance'] = PID_output_gap_variances\n",
    "    results['interest_variance'] = PID_interest_variances\n",
    "    results['loss'] = PID_loss_of_lambda\n",
    "    results['coefficients'] = PID_coefficients_array\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    # Saving DataFrame to nameOfModel.csv file\n",
    "    results_df.to_csv(nameOfModel + \".csv\", index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A normalized version of the createFrontier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizedCreateFrontier(modelNum):\n",
    "    taylor_coeff = [1.5, 0, 0.5, 0] # Taylor rule coefficients.\n",
    "    hamilton_coeff = [1.42,-1.20,0.5,-0.48] # Hamilton fit coefficients.\n",
    "    weights = np.arange(0, 1.01, .25) # Values for lambda\n",
    "    interest_coeff = 1.0 # Coefficient on the interest_rate loss term (for constrained optimization)\n",
    "    VarTarget = [16.0, 1.0, 1.0] # Normalizing coefficients for PID_loss on inital pass.\n",
    "    \n",
    "    mylambdatemp = 0.0 # Something to put in the lambda variable when calculating the target rate variance.\n",
    "    myfoo = myPID(taylor_coeff, mylambdatemp, interest_coeff, modelNum, VarTarget) # Run with Hamilton coefficients to generate target rate variance.\n",
    "    VarTarget[0] = unconditionalVariances()['interest'][0] # Normalizing variance for interest rate\n",
    "    VarTarget[1] = unconditionalVariances()['inflation'][0] # Normalizing variance for inflation\n",
    "    VarTarget[2] = unconditionalVariances()['outputgap'][0] # Normalizing variance for output gap\n",
    "    print(\"The normalizing variances are\", VarTarget) # Print normalizing variance.\n",
    "    \n",
    "    PID_inflation_variances = []\n",
    "    PID_output_gap_variances = []\n",
    "    PID_interest_variances = []\n",
    "    PID_loss_of_lambda = []\n",
    "    PID_coefficients_array = []\n",
    "    \n",
    "    # Starting with the Taylor rule coefficients\n",
    "    current_coeff = taylor_coeff\n",
    "\n",
    "    for lambda_value in weights:\n",
    "        PID_result = scipy.optimize.minimize(myPID, current_coeff, \\\n",
    "                                    args=(lambda_value, interest_coeff, modelNum, VarTarget), method='Nelder-Mead')\n",
    "        current_coeff = PID_result.x\n",
    "        PID_coefficients_array.append(current_coeff)\n",
    "        PID_inflation_variances.append(PID_final_inflation_variance)\n",
    "        PID_output_gap_variances.append(PID_final_output_gap_variance)\n",
    "        PID_interest_variances.append(PID_final_interest_rate_variance)\n",
    "        PID_loss_of_lambda.append(currentPIDLoss)\n",
    "    \n",
    "    # The code below plots the frontier\n",
    "    nameOfModel = getModelName()\n",
    "    \n",
    "    SD_inflation_scatter = np.asarray([np.sqrt(i) for i in PID_inflation_variances])\n",
    "    SD_output_gap_scatter = np.asarray([np.sqrt(i) for i in PID_output_gap_variances])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(SD_inflation_scatter, SD_output_gap_scatter, color=\"red\")\n",
    "\n",
    "    for i in range(0, len(weights)):\n",
    "        ax.annotate(weights[i], (SD_inflation_scatter[i], SD_output_gap_scatter[i]))\n",
    "\n",
    "    ax.set_xlabel('$\\sigma_{\\pi}$', fontsize=10)\n",
    "    ax.set_ylabel('$\\sigma_{y}$', fontsize=10)\n",
    "    ax.set_title('Policy Frontiers for Different Weights, Model: ' + nameOfModel, fontsize=14)\n",
    "    saveFileName = nameOfModel + '.pdf'\n",
    "    plt.savefig(saveFileName, bbox_inches='tight')\n",
    "    \n",
    "    # The code below saves the results of the all the optimizations with the appropriate lambdas into a DataFrame\n",
    "    results = dict()\n",
    "    results['lambdas'] = weights\n",
    "    results['inflation_variance'] = PID_inflation_variances\n",
    "    results['output_gap_variance'] = PID_output_gap_variances\n",
    "    results['interest_variance'] = PID_interest_variances\n",
    "    results['loss'] = PID_loss_of_lambda\n",
    "    results['coefficients'] = PID_coefficients_array\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    # Saving DataFrame to nameOfModel.csv file\n",
    "    results_df.to_csv(str(modelNum) + \"_\" + nameOfModel + \".csv\", index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PID Loss 95.195944 lambda = 0.0\n",
      "The target rate variances are 22.712\n",
      "Total PID Loss 95.195944 lambda = 0.0\n",
      "Total PID Loss 94.11063600000003 lambda = 0.0\n",
      "Total PID Loss 204.34187600000007 lambda = 0.0\n",
      "Total PID Loss 59.523569 lambda = 0.0\n",
      "Total PID Loss 217.61163600000006 lambda = 0.0\n",
      "Total PID Loss 48.96712099999999 lambda = 0.0\n",
      "Total PID Loss 37.605456 lambda = 0.0\n",
      "Total PID Loss 51.322900000000004 lambda = 0.0\n",
      "Total PID Loss 44.77368899999999 lambda = 0.0\n",
      "Total PID Loss 34.554169000000016 lambda = 0.0\n",
      "Total PID Loss 49.307216 lambda = 0.0\n",
      "Total PID Loss 50.12727600000001 lambda = 0.0\n",
      "Total PID Loss 39.98072899999999 lambda = 0.0\n",
      "Total PID Loss 68.71452099999998 lambda = 0.0\n",
      "Total PID Loss 39.029225000000004 lambda = 0.0\n",
      "Total PID Loss 59.40322500000002 lambda = 0.0\n",
      "Total PID Loss 37.381168999999986 lambda = 0.0\n",
      "Total PID Loss 44.092023999999995 lambda = 0.0\n",
      "Total PID Loss 35.516025000000006 lambda = 0.0\n",
      "Total PID Loss 41.11124899999999 lambda = 0.0\n",
      "Total PID Loss 35.71925599999999 lambda = 0.0\n",
      "Total PID Loss 45.56991600000002 lambda = 0.0\n",
      "Total PID Loss 34.71488899999999 lambda = 0.0\n",
      "Total PID Loss 36.69599999999999 lambda = 0.0\n",
      "Total PID Loss 34.77529599999998 lambda = 0.0\n",
      "Total PID Loss 37.97520899999999 lambda = 0.0\n",
      "Total PID Loss 34.700835999999995 lambda = 0.0\n",
      "Total PID Loss 34.1439 lambda = 0.0\n",
      "Total PID Loss 35.04622499999999 lambda = 0.0\n",
      "Total PID Loss 35.15002499999999 lambda = 0.0\n",
      "Total PID Loss 34.28959999999999 lambda = 0.0\n",
      "Total PID Loss 34.900600000000004 lambda = 0.0\n",
      "Total PID Loss 34.290684000000006 lambda = 0.0\n",
      "Total PID Loss 34.59404900000001 lambda = 0.0\n",
      "Total PID Loss 34.23416900000001 lambda = 0.0\n",
      "Total PID Loss 34.65600400000001 lambda = 0.0\n",
      "Total PID Loss 34.23055599999999 lambda = 0.0\n",
      "Total PID Loss 34.07022500000001 lambda = 0.0\n",
      "Total PID Loss 34.177464000000015 lambda = 0.0\n",
      "Total PID Loss 34.10337600000001 lambda = 0.0\n",
      "Total PID Loss 33.881584000000004 lambda = 0.0\n",
      "Total PID Loss 33.819649 lambda = 0.0\n",
      "Total PID Loss 33.989081 lambda = 0.0\n",
      "Total PID Loss 33.693943999999995 lambda = 0.0\n",
      "Total PID Loss 33.657600999999985 lambda = 0.0\n",
      "Total PID Loss 33.529503999999996 lambda = 0.0\n",
      "Total PID Loss 33.539103999999995 lambda = 0.0\n",
      "Total PID Loss 33.448704 lambda = 0.0\n",
      "Total PID Loss 33.595400000000005 lambda = 0.0\n",
      "Total PID Loss 33.776723999999994 lambda = 0.0\n",
      "Total PID Loss 33.32540900000001 lambda = 0.0\n",
      "Total PID Loss 33.48525600000001 lambda = 0.0\n",
      "Total PID Loss 33.449056 lambda = 0.0\n",
      "Total PID Loss 33.125364 lambda = 0.0\n",
      "Total PID Loss 33.17362500000001 lambda = 0.0\n",
      "Total PID Loss 33.04004900000001 lambda = 0.0\n",
      "Total PID Loss 33.122776 lambda = 0.0\n",
      "Total PID Loss 33.02392099999998 lambda = 0.0\n",
      "Total PID Loss 33.254644000000006 lambda = 0.0\n",
      "Total PID Loss 32.74362500000001 lambda = 0.0\n",
      "Total PID Loss 32.75040900000001 lambda = 0.0\n",
      "Total PID Loss 33.187096 lambda = 0.0\n",
      "Total PID Loss 32.94306399999999 lambda = 0.0\n",
      "Total PID Loss 32.554161 lambda = 0.0\n",
      "Total PID Loss 32.32802500000001 lambda = 0.0\n",
      "Total PID Loss 32.792401 lambda = 0.0\n",
      "Total PID Loss 32.246928999999994 lambda = 0.0\n",
      "Total PID Loss 32.03502499999999 lambda = 0.0\n",
      "Total PID Loss 32.782025000000004 lambda = 0.0\n",
      "Total PID Loss 32.45020399999999 lambda = 0.0\n",
      "Total PID Loss 32.70000400000001 lambda = 0.0\n",
      "Total PID Loss 31.882000000000005 lambda = 0.0\n",
      "Total PID Loss 31.737569 lambda = 0.0\n",
      "Total PID Loss 31.919456000000004 lambda = 0.0\n",
      "Total PID Loss 31.383463999999993 lambda = 0.0\n",
      "Total PID Loss 31.194368999999995 lambda = 0.0\n",
      "Total PID Loss 31.046849 lambda = 0.0\n",
      "Total PID Loss 31.208448999999995 lambda = 0.0\n",
      "Total PID Loss 30.594625 lambda = 0.0\n",
      "Total PID Loss 30.180316000000005 lambda = 0.0\n",
      "Total PID Loss 30.833328999999996 lambda = 0.0\n",
      "Total PID Loss 29.574529 lambda = 0.0\n",
      "Total PID Loss 28.980225000000004 lambda = 0.0\n",
      "Total PID Loss 29.774009 lambda = 0.0\n",
      "Total PID Loss 28.718196 lambda = 0.0\n",
      "Total PID Loss 27.76000099999999 lambda = 0.0\n",
      "Total PID Loss 27.269240999999997 lambda = 0.0\n",
      "Total PID Loss 665.4922010000003 lambda = 0.0\n",
      "Total PID Loss 185.961721 lambda = 0.0\n",
      "Total PID Loss 29.081649 lambda = 0.0\n",
      "Total PID Loss 28.548025000000003 lambda = 0.0\n",
      "Total PID Loss 139.46012900000002 lambda = 0.0\n",
      "Total PID Loss 28.371721 lambda = 0.0\n",
      "Total PID Loss 576.373784 lambda = 0.0\n",
      "Total PID Loss 28.36996899999999 lambda = 0.0\n",
      "Total PID Loss 28.857999999999993 lambda = 0.0\n",
      "Total PID Loss 27.956280999999986 lambda = 0.0\n",
      "Total PID Loss 26.92002500000001 lambda = 0.0\n",
      "Total PID Loss 470.31645599999996 lambda = 0.0\n",
      "Total PID Loss 461.2330250000001 lambda = 0.0\n",
      "Total PID Loss 27.85459999999999 lambda = 0.0\n",
      "Total PID Loss 26.581009 lambda = 0.0\n",
      "Total PID Loss 76.8374 lambda = 0.0\n",
      "Total PID Loss 266.374216 lambda = 0.0\n",
      "Total PID Loss 27.372681 lambda = 0.0\n",
      "Total PID Loss 27.286001000000013 lambda = 0.0\n",
      "Total PID Loss 64.671225 lambda = 0.0\n",
      "Total PID Loss 27.028363999999996 lambda = 0.0\n",
      "Total PID Loss 27.402196000000004 lambda = 0.0\n",
      "Total PID Loss 26.91142400000001 lambda = 0.0\n",
      "Total PID Loss 37.160225000000004 lambda = 0.0\n",
      "Total PID Loss 26.943603999999993 lambda = 0.0\n",
      "Total PID Loss 32.26865600000002 lambda = 0.0\n",
      "Total PID Loss 26.858809000000008 lambda = 0.0\n",
      "Total PID Loss 26.852089000000007 lambda = 0.0\n",
      "Total PID Loss 27.028129000000007 lambda = 0.0\n",
      "Total PID Loss 26.654484000000014 lambda = 0.0\n",
      "Total PID Loss 27.733183999999994 lambda = 0.0\n",
      "Total PID Loss 26.711225000000006 lambda = 0.0\n",
      "Total PID Loss 40.462025 lambda = 0.0\n",
      "Total PID Loss 26.719156000000012 lambda = 0.0\n",
      "Total PID Loss 26.849329 lambda = 0.0\n",
      "Total PID Loss 26.732303999999992 lambda = 0.0\n",
      "Total PID Loss 26.55704900000001 lambda = 0.0\n",
      "Total PID Loss 26.852089000000007 lambda = 0.0\n",
      "Total PID Loss 26.460009 lambda = 0.0\n",
      "Total PID Loss 30.53057600000001 lambda = 0.0\n",
      "Total PID Loss 29.778924000000007 lambda = 0.0\n",
      "Total PID Loss 26.612089000000008 lambda = 0.0\n",
      "Total PID Loss 26.530195999999997 lambda = 0.0\n",
      "Total PID Loss 26.442440999999995 lambda = 0.0\n",
      "Total PID Loss 28.527680999999998 lambda = 0.0\n",
      "Total PID Loss 26.799599999999995 lambda = 0.0\n",
      "Total PID Loss 26.52200100000001 lambda = 0.0\n",
      "Total PID Loss 26.530081000000003 lambda = 0.0\n",
      "Total PID Loss 26.644036 lambda = 0.0\n",
      "Total PID Loss 26.492009000000003 lambda = 0.0\n",
      "Total PID Loss 26.55401600000001 lambda = 0.0\n",
      "Total PID Loss 26.478004000000006 lambda = 0.0\n",
      "Total PID Loss 26.51304899999999 lambda = 0.0\n",
      "Total PID Loss 26.422081 lambda = 0.0\n",
      "Total PID Loss 26.841576000000003 lambda = 0.0\n",
      "Total PID Loss 26.466009 lambda = 0.0\n",
      "Total PID Loss 26.430121000000003 lambda = 0.0\n",
      "Total PID Loss 26.776484000000014 lambda = 0.0\n",
      "Total PID Loss 26.442064000000002 lambda = 0.0\n",
      "Total PID Loss 26.447440999999998 lambda = 0.0\n",
      "Total PID Loss 26.432323999999994 lambda = 0.0\n",
      "Total PID Loss 26.421004000000007 lambda = 0.0\n",
      "Total PID Loss 26.439144000000006 lambda = 0.0\n",
      "Total PID Loss 26.46809999999999 lambda = 0.0\n",
      "Total PID Loss 26.42809999999999 lambda = 0.0\n",
      "Total PID Loss 26.443036 lambda = 0.0\n",
      "Total PID Loss 26.423196000000004 lambda = 0.0\n",
      "Total PID Loss 26.445004000000008 lambda = 0.0\n",
      "Total PID Loss 26.420121000000005 lambda = 0.0\n",
      "Total PID Loss 26.409081 lambda = 0.0\n",
      "Total PID Loss 26.397081 lambda = 0.0\n",
      "Total PID Loss 26.411001000000013 lambda = 0.0\n",
      "Total PID Loss 26.429003999999996 lambda = 0.0\n",
      "Total PID Loss 26.413064000000006 lambda = 0.0\n",
      "Total PID Loss 26.413143999999996 lambda = 0.0\n",
      "Total PID Loss 26.415015999999994 lambda = 0.0\n",
      "Total PID Loss 26.397036 lambda = 0.0\n",
      "Total PID Loss 26.413000999999987 lambda = 0.0\n",
      "Total PID Loss 26.399000999999984 lambda = 0.0\n",
      "Total PID Loss 26.525003999999992 lambda = 0.0\n",
      "Total PID Loss 26.403004000000006 lambda = 0.0\n",
      "Total PID Loss 26.452015999999993 lambda = 0.0\n",
      "Total PID Loss 26.402009 lambda = 0.0\n",
      "Total PID Loss 26.455008999999997 lambda = 0.0\n",
      "Total PID Loss 26.402009 lambda = 0.0\n",
      "Total PID Loss 26.408025000000006 lambda = 0.0\n",
      "Total PID Loss 26.39801599999999 lambda = 0.0\n",
      "Total PID Loss 26.432025000000007 lambda = 0.0\n",
      "Total PID Loss 26.39801599999999 lambda = 0.0\n",
      "Total PID Loss 26.405121000000005 lambda = 0.0\n",
      "Total PID Loss 26.394008999999997 lambda = 0.0\n",
      "Total PID Loss 26.39904899999999 lambda = 0.0\n",
      "Total PID Loss 26.396025000000005 lambda = 0.0\n",
      "Total PID Loss 26.403048999999992 lambda = 0.0\n",
      "Total PID Loss 26.396025000000005 lambda = 0.0\n",
      "Total PID Loss 26.398 lambda = 0.0\n",
      "Total PID Loss 26.395048999999993 lambda = 0.0\n",
      "Total PID Loss 26.400008999999997 lambda = 0.0\n",
      "Total PID Loss 26.39902500000001 lambda = 0.0\n",
      "Total PID Loss 26.395025000000008 lambda = 0.0\n",
      "Total PID Loss 26.39501599999999 lambda = 0.0\n",
      "Total PID Loss 26.39501599999999 lambda = 0.0\n",
      "Total PID Loss 26.39901599999999 lambda = 0.0\n",
      "Total PID Loss 26.397008999999997 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.397008999999997 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.394015999999993 lambda = 0.0\n",
      "Total PID Loss 26.393015999999996 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.397008999999997 lambda = 0.0\n",
      "Total PID Loss 26.394015999999993 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.394015999999993 lambda = 0.0\n",
      "Total PID Loss 26.398009000000002 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.393015999999996 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n",
      "Total PID Loss 26.393015999999996 lambda = 0.0\n",
      "Total PID Loss 26.399009 lambda = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Model Number 15 - US FRB03\n",
    "m15results = createFrontier(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m15resultsnorm = normalizedCreateFrontier(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Optimization across models/regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PID_loss_array = []\n",
    "PID_final_inflation_variance = 0\n",
    "PID_final_output_gap_variance = 0\n",
    "PID_final_interest_rate_variance = 0\n",
    "currentPIDLoss = 0\n",
    "\n",
    "# Without normalization\n",
    "def multiplePID(selected_coeff, lambdaVal, coeffInterest, modelNums, normalizing_matrix):\n",
    "    global PID_final_inflation_variance\n",
    "    global PID_final_output_gap_variance\n",
    "    global PID_final_interest_rate_variance\n",
    "    global currentPIDLoss\n",
    "\n",
    "    coefficients = [1, 0, 0, 0, \n",
    "                    abs(selected_coeff[0])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    (abs(selected_coeff[0])+selected_coeff[1])/4, \n",
    "                    selected_coeff[1]/4, \n",
    "                    0, 0, 0, 0, \n",
    "                    selected_coeff[2], selected_coeff[3], 0, \n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.25]\n",
    "    \n",
    "    # Initializing the PID loss to 0\n",
    "    n = len(modelNums)\n",
    "    weight = 1.0/n\n",
    "    sumOfInterestVariances = 0\n",
    "    sumOfInflationVariances = 0\n",
    "    sumOfOutputGapVariances = 0\n",
    "    \n",
    "    # Running each model in modelNums\n",
    "    for i in range(n):\n",
    "        scipy.io.savemat('variables.mat', dict(coefficients=coefficients, modelNumber = modelNums[i])) # Input for MMB.\n",
    "        eng.MMB(nargout = 0) # Run MMB\n",
    "        variances = unconditionalVariances()\n",
    "        \n",
    "        # Inverse weight each of these by Taylor variances to `normalize`\n",
    "        interest_rate_variance = ((variances['interest'][0] - normalizing_matrix[i][0])/normalizing_matrix[i][0])**2\n",
    "        inflation_variance = variances['inflation'][0]/normalizing_matrix[i][1]\n",
    "        output_gap_variance = variances['outputgap'][0]/normalizing_matrix[i][2]\n",
    "        \n",
    "        sumOfInterestVariances += interest_rate_variance\n",
    "        sumOfInflationVariances += inflation_variance\n",
    "        sumOfOutputGapVariances += output_gap_variance\n",
    "    \n",
    "    PID_final_inflation_variance = 1.0/n * sumOfInflationVariances\n",
    "    PID_final_output_gap_variance = 1.0/n * sumOfOutputGapVariances\n",
    "    PID_final_interest_rate_variance = 1.0/n * sumOfInterestVariances\n",
    "    \n",
    "    PID_loss = ((PID_final_interest_rate_variance) * coeffInterest \n",
    "                + (PID_final_inflation_variance) * lambdaVal\n",
    "                + (PID_final_output_gap_variance) * (1 - lambdaVal))\n",
    "    \n",
    "    print(\"Current PID Loss\", PID_loss, \"lambda =\", lambdaVal)\n",
    "    PID_loss_array.append(PID_loss)\n",
    "    currentPIDLoss = PID_loss\n",
    "    \n",
    "    return PID_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createGroupedFrontier(modelNums):\n",
    "    taylor_coeff = [1.5, 0, 0.5, 0] # Taylor rule coefficients.\n",
    "    hamilton_coeff = [1.42,-1.20,0.5,-0.48] # Hamilton fit coefficients.\n",
    "    weights = np.arange(0, 1.01, .25) # Values for lambda\n",
    "    interest_coeff = 1.0 # Coefficient on the interest_rate loss term (for constrained optimization)\n",
    "    VarTarget = [16.0, 1.0, 1.0] # Normalizing coefficients for PID_loss on inital pass.\n",
    "    \n",
    "    normalizing_matrix = []\n",
    "    mylambdatemp = 0.0 # Something to put in the lambda variable when calculating the target rate variance.\n",
    "    \n",
    "    for modelNum in modelNums:\n",
    "        # Run with Taylor coefficients to generate target rate variance.\n",
    "        myfoo = myPID(taylor_coeff, mylambdatemp, interest_coeff, modelNum, VarTarget)\n",
    "        interest_m = unconditionalVariances()['interest'][0] # Normalizing variance for interest rate\n",
    "        inflation_m = unconditionalVariances()['inflation'][0] # Normalizing variance for inflation\n",
    "        output_gap_m = unconditionalVariances()['outputgap'][0] # Normalizing variance for output gap\n",
    "        normalizing_matrix.append([interest_m, inflation_m, output_gap_m])\n",
    "    \n",
    "    PID_inflation_variances = []\n",
    "    PID_output_gap_variances = []\n",
    "    PID_interest_variances = []\n",
    "    PID_loss_of_lambda = []\n",
    "    PID_coefficients_array = []\n",
    "    \n",
    "    # Starting with the Taylor rule coefficients\n",
    "    current_coeff = taylor_coeff\n",
    "\n",
    "    for lambda_value in weights:\n",
    "        PID_result = scipy.optimize.minimize(multiplePID, current_coeff, \\\n",
    "                                    args=(lambda_value, interest_coeff, modelNums, normalizing_matrix), method='Nelder-Mead')\n",
    "        current_coeff = PID_result.x\n",
    "        \n",
    "        # Appending to the results\n",
    "        PID_coefficients_array.append(current_coeff)\n",
    "        PID_inflation_variances.append(PID_final_inflation_variance)\n",
    "        PID_output_gap_variances.append(PID_final_output_gap_variance)\n",
    "        PID_interest_variances.append(PID_final_interest_rate_variance)\n",
    "        PID_loss_of_lambda.append(currentPIDLoss)\n",
    "    \n",
    "    # The code below plots the frontier\n",
    "    modelNumbers = '_'.join(str(x) for x in modelNums)\n",
    "    \n",
    "    SD_inflation_scatter = np.asarray([np.sqrt(i) for i in PID_inflation_variances])\n",
    "    SD_output_gap_scatter = np.asarray([np.sqrt(i) for i in PID_output_gap_variances])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(SD_inflation_scatter, SD_output_gap_scatter, color=\"red\")\n",
    "\n",
    "    for i in range(0, len(weights)):\n",
    "        ax.annotate(weights[i], (SD_inflation_scatter[i], SD_output_gap_scatter[i]))\n",
    "\n",
    "    ax.set_xlabel('$\\sigma_{\\pi}$', fontsize=10)\n",
    "    ax.set_ylabel('$\\sigma_{y}$', fontsize=10)\n",
    "    ax.set_title('Policy Frontiers for Different Weights, Multiple Models: ' + modelNumbers, fontsize=14)\n",
    "    saveFileName = \"Multiple_\" + modelNumbers + '.pdf'\n",
    "    plt.savefig(saveFileName, bbox_inches='tight')\n",
    "    \n",
    "    # The code below saves the results of the all the optimizations with the appropriate lambdas into a DataFrame\n",
    "    results = dict()\n",
    "    results['lambdas'] = weights\n",
    "    results['inflation_variance'] = PID_inflation_variances\n",
    "    results['output_gap_variance'] = PID_output_gap_variances\n",
    "    results['interest_variance'] = PID_interest_variances\n",
    "    results['loss'] = PID_loss_of_lambda\n",
    "    results['pi_t'] = [array[0] for array in PID_coefficients_array]\n",
    "    results['pi_t-1'] = [array[1] for array in PID_coefficients_array]\n",
    "    results['y_t'] = [array[2] for array in PID_coefficients_array]\n",
    "    results['y_t-1'] = [array[3] for array in PID_coefficients_array]\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    \n",
    "    # Saving DataFrame to nameOfModel.csv file\n",
    "    results_df.to_csv(\"Multiple_\" + modelNumbers + \".csv\", index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates a multiple model (grouped Frontier) for models 107, 108\n",
    "To run the model please call `createGroupedFrontier` with a `list` of the required model numbers:\n",
    "\n",
    "For example: `createGroupedFrontier([107, 108])`\n",
    "\n",
    "After the optimization has completed, your folder (wherever this notebook Structured_Frontier is saved) should contain a pdf image eg. `Multiple_107_108.pdf` and a .csv file `Multiple_107_108.csv` with the results of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myPID Loss 36770968.01 lambda = 0.0\n",
      "myPID Loss 29729117.639999997 lambda = 0.0\n",
      "Current PID Loss 1.0 lambda = 0.0\n",
      "Current PID Loss 1.0137334293238116 lambda = 0.0\n",
      "Current PID Loss 1.000014222474835 lambda = 0.0\n",
      "Current PID Loss 0.9981391275275352 lambda = 0.0\n",
      "Current PID Loss 0.9999337486780276 lambda = 0.0\n",
      "Current PID Loss 1.001977519451962 lambda = 0.0\n",
      "Current PID Loss 0.9971798333757138 lambda = 0.0\n",
      "Current PID Loss 0.9976841965963403 lambda = 0.0\n",
      "Current PID Loss 1.001237936033166 lambda = 0.0\n",
      "Current PID Loss 0.9977401989538215 lambda = 0.0\n",
      "Current PID Loss 1.0032449495716622 lambda = 0.0\n",
      "Current PID Loss 0.9975595679503603 lambda = 0.0\n",
      "Current PID Loss 0.99770507379051 lambda = 0.0\n",
      "Current PID Loss 0.9972264381725131 lambda = 0.0\n",
      "Current PID Loss 0.9998658611050272 lambda = 0.0\n",
      "Current PID Loss 0.9969620431892837 lambda = 0.0\n",
      "Current PID Loss 0.9971355976978025 lambda = 0.0\n",
      "Current PID Loss 0.9971508244620254 lambda = 0.0\n",
      "Current PID Loss 0.9968561510394288 lambda = 0.0\n",
      "Current PID Loss 0.9975824809353669 lambda = 0.0\n",
      "Current PID Loss 0.9973065098603695 lambda = 0.0\n",
      "Current PID Loss 0.9967987851491305 lambda = 0.0\n",
      "Current PID Loss 0.9989288956907966 lambda = 0.0\n",
      "Current PID Loss 0.9966636405867954 lambda = 0.0\n",
      "Current PID Loss 0.9969873416841385 lambda = 0.0\n",
      "Current PID Loss 0.9967813444469172 lambda = 0.0\n",
      "Current PID Loss 0.9967125919279896 lambda = 0.0\n",
      "Current PID Loss 0.9977069720156975 lambda = 0.0\n",
      "Current PID Loss 0.996657512367131 lambda = 0.0\n",
      "Current PID Loss 0.9965238751314358 lambda = 0.0\n",
      "Current PID Loss 0.9963811777725815 lambda = 0.0\n",
      "Current PID Loss 0.9963735242348778 lambda = 0.0\n",
      "Current PID Loss 0.9961648511782524 lambda = 0.0\n",
      "Current PID Loss 0.9963904681917871 lambda = 0.0\n",
      "Current PID Loss 0.9961803051222344 lambda = 0.0\n",
      "Current PID Loss 0.9958266550884168 lambda = 0.0\n",
      "Current PID Loss 0.9954136041136641 lambda = 0.0\n",
      "Current PID Loss 0.9957631477309047 lambda = 0.0\n",
      "Current PID Loss 0.9952934967836577 lambda = 0.0\n",
      "Current PID Loss 0.9947166668354601 lambda = 0.0\n",
      "Current PID Loss 0.995125526263429 lambda = 0.0\n",
      "Current PID Loss 0.994202621043862 lambda = 0.0\n",
      "Current PID Loss 0.9931261036035274 lambda = 0.0\n",
      "Current PID Loss 0.9932441215248664 lambda = 0.0\n",
      "Current PID Loss 0.9925161371095741 lambda = 0.0\n",
      "Current PID Loss 0.9909091562522633 lambda = 0.0\n",
      "Current PID Loss 0.9906445403602668 lambda = 0.0\n",
      "Current PID Loss 0.9880465511606362 lambda = 0.0\n",
      "Current PID Loss 0.9873897894833664 lambda = 0.0\n",
      "Current PID Loss 0.983416220749139 lambda = 0.0\n",
      "Current PID Loss 0.9832568059378581 lambda = 0.0\n",
      "Current PID Loss 0.9777597259021722 lambda = 0.0\n",
      "Current PID Loss 0.9711964290432694 lambda = 0.0\n",
      "Current PID Loss 1.8290436257740312 lambda = 0.0\n",
      "Current PID Loss 0.9664473962208231 lambda = 0.0\n",
      "Current PID Loss 3.629639447023088 lambda = 0.0\n",
      "Current PID Loss 102.61042402046087 lambda = 0.0\n",
      "Current PID Loss 0.9807678288417007 lambda = 0.0\n",
      "Current PID Loss 0.9540012360978761 lambda = 0.0\n",
      "Current PID Loss 1.476030119112743 lambda = 0.0\n",
      "Current PID Loss 9.356737030942881 lambda = 0.0\n",
      "Current PID Loss 0.9736502645799165 lambda = 0.0\n",
      "Current PID Loss 0.9439293395345424 lambda = 0.0\n",
      "Current PID Loss 0.936412749474667 lambda = 0.0\n",
      "Current PID Loss 1.2887713408096744 lambda = 0.0\n",
      "Current PID Loss 0.9644456963316569 lambda = 0.0\n",
      "Current PID Loss 1.016866693756087 lambda = 0.0\n",
      "Current PID Loss 0.9624170987244651 lambda = 0.0\n",
      "Current PID Loss 0.9453040571325793 lambda = 0.0\n",
      "Current PID Loss 0.9467693189744267 lambda = 0.0\n",
      "Current PID Loss 1.4600225077779025 lambda = 0.0\n",
      "Current PID Loss 0.949480486657173 lambda = 0.0\n",
      "Current PID Loss 0.8943422383695961 lambda = 0.0\n",
      "Current PID Loss 1.1020706212322824 lambda = 0.0\n",
      "Current PID Loss 1.2529093838569494 lambda = 0.0\n",
      "Current PID Loss 0.9347001237250969 lambda = 0.0\n",
      "Current PID Loss 0.9363930932291908 lambda = 0.0\n",
      "Current PID Loss 1.377214661714587 lambda = 0.0\n",
      "Current PID Loss 0.9329903704726724 lambda = 0.0\n",
      "Current PID Loss 0.9460613897740455 lambda = 0.0\n",
      "Current PID Loss 0.9157554196094254 lambda = 0.0\n",
      "Current PID Loss 0.9477423077931 lambda = 0.0\n",
      "Current PID Loss 0.9281501119658639 lambda = 0.0\n",
      "Current PID Loss 0.9003407298026562 lambda = 0.0\n",
      "Current PID Loss 0.8935455943740092 lambda = 0.0\n",
      "Current PID Loss 1.8117147374832685 lambda = 0.0\n",
      "Current PID Loss 1.8163820763649678 lambda = 0.0\n",
      "Current PID Loss 0.9135350701679478 lambda = 0.0\n",
      "Current PID Loss 0.8802841617993464 lambda = 0.0\n",
      "Current PID Loss 0.8807465219419048 lambda = 0.0\n",
      "Current PID Loss 0.9161323033847264 lambda = 0.0\n",
      "Current PID Loss 0.9008208145146422 lambda = 0.0\n",
      "Current PID Loss 0.8784068524347871 lambda = 0.0\n",
      "Current PID Loss 0.9161323033847264 lambda = 0.0\n",
      "Current PID Loss 1.0442520650153022 lambda = 0.0\n",
      "Current PID Loss 0.8899427578329108 lambda = 0.0\n",
      "Current PID Loss 0.8736474373006968 lambda = 0.0\n",
      "Current PID Loss 0.8854719975114561 lambda = 0.0\n",
      "Current PID Loss 0.8855607308478306 lambda = 0.0\n",
      "Current PID Loss 0.880199562097146 lambda = 0.0\n",
      "Current PID Loss 0.9153446653355267 lambda = 0.0\n",
      "Current PID Loss 0.8782615463957073 lambda = 0.0\n",
      "Current PID Loss 0.8854010140793546 lambda = 0.0\n",
      "Current PID Loss 0.8760855287780089 lambda = 0.0\n",
      "Current PID Loss 0.8857169881942942 lambda = 0.0\n",
      "Current PID Loss 0.8740679896168119 lambda = 0.0\n",
      "Current PID Loss 0.8817052198537526 lambda = 0.0\n",
      "Current PID Loss 0.8748091276394315 lambda = 0.0\n",
      "Current PID Loss 0.8779060971180626 lambda = 0.0\n",
      "Current PID Loss 0.8742764682490657 lambda = 0.0\n",
      "Current PID Loss 0.8780762255574335 lambda = 0.0\n",
      "Current PID Loss 0.8744023861597279 lambda = 0.0\n",
      "Current PID Loss 0.8743227508052949 lambda = 0.0\n",
      "Current PID Loss 0.8762134831154381 lambda = 0.0\n",
      "Current PID Loss 0.8737296940980576 lambda = 0.0\n",
      "Current PID Loss 0.8744506866981429 lambda = 0.0\n",
      "Current PID Loss 0.8737451503185684 lambda = 0.0\n",
      "Current PID Loss 0.8747708420402671 lambda = 0.0\n",
      "Current PID Loss 0.8737122130709944 lambda = 0.0\n",
      "Current PID Loss 0.8741937224264269 lambda = 0.0\n",
      "Current PID Loss 0.8736987278365906 lambda = 0.0\n",
      "Current PID Loss 0.8738721050418531 lambda = 0.0\n",
      "Current PID Loss 0.8736181432931182 lambda = 0.0\n",
      "Current PID Loss 0.8738608494353165 lambda = 0.0\n",
      "Current PID Loss 0.8736206439311893 lambda = 0.0\n",
      "Current PID Loss 0.8737602000565623 lambda = 0.0\n",
      "Current PID Loss 0.8736301925018647 lambda = 0.0\n",
      "Current PID Loss 0.8737736112483089 lambda = 0.0\n",
      "Current PID Loss 0.8736215679284979 lambda = 0.0\n",
      "Current PID Loss 0.8736899555143444 lambda = 0.0\n",
      "Current PID Loss 0.8736239879258095 lambda = 0.0\n",
      "Current PID Loss 0.8736500274828807 lambda = 0.0\n",
      "Current PID Loss 0.8736000595958168 lambda = 0.0\n",
      "Current PID Loss 0.873637464881794 lambda = 0.0\n",
      "Current PID Loss 0.8736075558749846 lambda = 0.0\n",
      "Current PID Loss 0.8736177957116009 lambda = 0.0\n",
      "Current PID Loss 0.8736355013837299 lambda = 0.0\n",
      "Current PID Loss 0.8736057853535096 lambda = 0.0\n",
      "Current PID Loss 0.8736714617321212 lambda = 0.0\n",
      "Current PID Loss 0.8736050787099763 lambda = 0.0\n",
      "Current PID Loss 0.8736068533351956 lambda = 0.0\n",
      "Current PID Loss 0.8736089278610252 lambda = 0.0\n",
      "Current PID Loss 0.8735982593548351 lambda = 0.0\n",
      "Current PID Loss 0.8736150976064082 lambda = 0.0\n",
      "Current PID Loss 0.8736095138632688 lambda = 0.0\n",
      "Current PID Loss 0.8736012359805271 lambda = 0.0\n",
      "Current PID Loss 0.8736085120229607 lambda = 0.0\n",
      "Current PID Loss 0.8736136927201461 lambda = 0.0\n",
      "Current PID Loss 0.87360821657531 lambda = 0.0\n",
      "Current PID Loss 0.873604287876081 lambda = 0.0\n",
      "Current PID Loss 0.8736010582158339 lambda = 0.0\n",
      "Current PID Loss 0.8736003589315092 lambda = 0.0\n",
      "Current PID Loss 0.8736114418294906 lambda = 0.0\n",
      "Current PID Loss 0.8736099894820147 lambda = 0.0\n",
      "Current PID Loss 0.8736061014109013 lambda = 0.0\n",
      "Current PID Loss 0.8736081333074251 lambda = 0.0\n",
      "Current PID Loss 0.8736094934444001 lambda = 0.0\n",
      "Current PID Loss 0.8736111903780969 lambda = 0.0\n",
      "Current PID Loss 0.8736055945922957 lambda = 0.0\n",
      "Current PID Loss 0.8735984083707308 lambda = 0.0\n",
      "Current PID Loss 0.8736041697315334 lambda = 0.0\n",
      "Current PID Loss 0.8736026358256144 lambda = 0.0\n",
      "Current PID Loss 0.8736066096185477 lambda = 0.0\n",
      "Current PID Loss 0.8735925763957316 lambda = 0.0\n",
      "Current PID Loss 0.8736040202557925 lambda = 0.0\n",
      "Current PID Loss 0.8736081963306515 lambda = 0.0\n",
      "Current PID Loss 0.8736075128775252 lambda = 0.0\n",
      "Current PID Loss 0.8735978371622335 lambda = 0.0\n",
      "Current PID Loss 0.8736034437064717 lambda = 0.0\n",
      "Current PID Loss 0.8735932023219984 lambda = 0.0\n",
      "Current PID Loss 1.02823555612658 lambda = 0.25\n",
      "Current PID Loss 1.0069348796128823 lambda = 0.25\n",
      "Current PID Loss 1.0227437537691935 lambda = 0.25\n",
      "Current PID Loss 1.070024967709423 lambda = 0.25\n",
      "Current PID Loss 1.0267915376890444 lambda = 0.25\n",
      "Current PID Loss 0.9970700714907861 lambda = 0.25\n",
      "Current PID Loss 0.9952905851259541 lambda = 0.25\n",
      "Current PID Loss 0.9936068836831453 lambda = 0.25\n",
      "Current PID Loss 0.9930179006125123 lambda = 0.25\n",
      "Current PID Loss 0.9952252078895156 lambda = 0.25\n",
      "Current PID Loss 1.0147734086901052 lambda = 0.25\n",
      "Current PID Loss 0.9983294918167284 lambda = 0.25\n",
      "Current PID Loss 1.0373038073374827 lambda = 0.25\n",
      "Current PID Loss 0.9928515773132044 lambda = 0.25\n",
      "Current PID Loss 0.9940651379223495 lambda = 0.25\n",
      "Current PID Loss 0.9907195883090077 lambda = 0.25\n",
      "Current PID Loss 0.9911558276622565 lambda = 0.25\n",
      "Current PID Loss 1.0004706652865356 lambda = 0.25\n",
      "Current PID Loss 0.9921873364237814 lambda = 0.25\n",
      "Current PID Loss 0.9911932049253516 lambda = 0.25\n",
      "Current PID Loss 0.990867224019779 lambda = 0.25\n",
      "Current PID Loss 0.9903815310074844 lambda = 0.25\n",
      "Current PID Loss 0.992288661089519 lambda = 0.25\n",
      "Current PID Loss 0.9884032331278234 lambda = 0.25\n",
      "Current PID Loss 0.9877752910799305 lambda = 0.25\n",
      "Current PID Loss 0.9915964898753193 lambda = 0.25\n",
      "Current PID Loss 0.989605674231058 lambda = 0.25\n",
      "Current PID Loss 0.9884788096024573 lambda = 0.25\n",
      "Current PID Loss 0.9879711139310352 lambda = 0.25\n",
      "Current PID Loss 0.986279189733502 lambda = 0.25\n",
      "Current PID Loss 0.9860217199809159 lambda = 0.25\n",
      "Current PID Loss 0.9839873014925469 lambda = 0.25\n",
      "Current PID Loss 0.9822123846775912 lambda = 0.25\n",
      "Current PID Loss 0.9846551493875098 lambda = 0.25\n",
      "Current PID Loss 0.9928758332979553 lambda = 0.25\n",
      "Current PID Loss 0.9850229610636216 lambda = 0.25\n",
      "Current PID Loss 0.9811010908128671 lambda = 0.25\n",
      "Current PID Loss 0.9787677721713469 lambda = 0.25\n",
      "Current PID Loss 0.9792145043049163 lambda = 0.25\n",
      "Current PID Loss 0.9792779222029717 lambda = 0.25\n",
      "Current PID Loss 0.9764982328987295 lambda = 0.25\n",
      "Current PID Loss 0.9749073532191748 lambda = 0.25\n",
      "Current PID Loss 0.9744108086752741 lambda = 0.25\n",
      "Current PID Loss 0.9727350567180865 lambda = 0.25\n",
      "Current PID Loss 0.9757036748562891 lambda = 0.25\n",
      "Current PID Loss 0.9729106847213775 lambda = 0.25\n",
      "Current PID Loss 0.9731315573291802 lambda = 0.25\n",
      "Current PID Loss 0.9717605057232235 lambda = 0.25\n",
      "Current PID Loss 0.9716330982755257 lambda = 0.25\n",
      "Current PID Loss 0.9715839597165311 lambda = 0.25\n",
      "Current PID Loss 0.9722540753912683 lambda = 0.25\n",
      "Current PID Loss 0.9726905069779397 lambda = 0.25\n",
      "Current PID Loss 0.9728250238426852 lambda = 0.25\n",
      "Current PID Loss 0.9720209879195612 lambda = 0.25\n",
      "Current PID Loss 0.9725865853395224 lambda = 0.25\n",
      "Current PID Loss 0.9720862629452056 lambda = 0.25\n",
      "Current PID Loss 0.9718030017859522 lambda = 0.25\n",
      "Current PID Loss 0.9729271822518892 lambda = 0.25\n",
      "Current PID Loss 0.9716609755060803 lambda = 0.25\n",
      "Current PID Loss 0.9722220575825342 lambda = 0.25\n",
      "Current PID Loss 0.9716019441380331 lambda = 0.25\n",
      "Current PID Loss 0.9721954637324492 lambda = 0.25\n",
      "Current PID Loss 0.9715288803627753 lambda = 0.25\n",
      "Current PID Loss 0.9716587031657866 lambda = 0.25\n",
      "Current PID Loss 0.971529731638636 lambda = 0.25\n",
      "Current PID Loss 0.9716033718672206 lambda = 0.25\n",
      "Current PID Loss 0.9715208152813611 lambda = 0.25\n",
      "Current PID Loss 0.9717821532611318 lambda = 0.25\n",
      "Current PID Loss 0.9715130486506316 lambda = 0.25\n",
      "Current PID Loss 0.9716305766219815 lambda = 0.25\n",
      "Current PID Loss 0.9715168735041803 lambda = 0.25\n",
      "Current PID Loss 0.9716066067637874 lambda = 0.25\n",
      "Current PID Loss 0.971492641371619 lambda = 0.25\n",
      "Current PID Loss 0.9715089776382515 lambda = 0.25\n",
      "Current PID Loss 0.9715344272596493 lambda = 0.25\n",
      "Current PID Loss 0.9715026533798008 lambda = 0.25\n",
      "Current PID Loss 0.9715302658632368 lambda = 0.25\n",
      "Current PID Loss 0.97149456605576 lambda = 0.25\n",
      "Current PID Loss 0.9715117240300797 lambda = 0.25\n",
      "Current PID Loss 0.971491374551007 lambda = 0.25\n",
      "Current PID Loss 0.9715287891570255 lambda = 0.25\n",
      "Current PID Loss 0.9714920730590758 lambda = 0.25\n",
      "Current PID Loss 0.971491497537424 lambda = 0.25\n",
      "Current PID Loss 0.971491258671398 lambda = 0.25\n",
      "Current PID Loss 0.9715084064367292 lambda = 0.25\n",
      "Current PID Loss 0.9715062107200394 lambda = 0.25\n",
      "Current PID Loss 0.9714883804507282 lambda = 0.25\n",
      "Current PID Loss 0.9714911683065627 lambda = 0.25\n",
      "Current PID Loss 0.97149570760339 lambda = 0.25\n",
      "Current PID Loss 0.9714972198673005 lambda = 0.25\n",
      "Current PID Loss 0.9714905859495684 lambda = 0.25\n",
      "Current PID Loss 0.9714926818906087 lambda = 0.25\n",
      "Current PID Loss 0.9714892174850348 lambda = 0.25\n",
      "Current PID Loss 0.9714991946325551 lambda = 0.25\n",
      "Current PID Loss 0.9714922358037319 lambda = 0.25\n",
      "Current PID Loss 0.9714940226949296 lambda = 0.25\n",
      "Current PID Loss 0.9715005899172289 lambda = 0.25\n",
      "Current PID Loss 0.9714932050358616 lambda = 0.25\n",
      "Current PID Loss 0.9714972167143073 lambda = 0.25\n",
      "Current PID Loss 0.9714949736794868 lambda = 0.25\n",
      "Current PID Loss 0.971493016727446 lambda = 0.25\n",
      "Current PID Loss 0.9714866499746182 lambda = 0.25\n",
      "Current PID Loss 0.9714938579166987 lambda = 0.25\n",
      "Current PID Loss 0.9714881577253005 lambda = 0.25\n",
      "Current PID Loss 0.9714933760626292 lambda = 0.25\n",
      "Current PID Loss 0.971488204394582 lambda = 0.25\n",
      "Current PID Loss 0.9714908488028066 lambda = 0.25\n",
      "Current PID Loss 0.9714876348582995 lambda = 0.25\n",
      "Current PID Loss 0.9714905109551067 lambda = 0.25\n",
      "Current PID Loss 0.9714877751094344 lambda = 0.25\n",
      "Current PID Loss 0.9714865072717944 lambda = 0.25\n",
      "Current PID Loss 0.9714863398407767 lambda = 0.25\n",
      "Current PID Loss 0.9714852623835821 lambda = 0.25\n",
      "Current PID Loss 0.9714932861907195 lambda = 0.25\n",
      "Current PID Loss 0.9714964468486436 lambda = 0.25\n",
      "Current PID Loss 0.971489458170149 lambda = 0.25\n",
      "Current PID Loss 0.9714974291694201 lambda = 0.25\n",
      "Current PID Loss 0.9714893731350063 lambda = 0.25\n",
      "Current PID Loss 0.971499984415997 lambda = 0.25\n",
      "Current PID Loss 0.9714892026173503 lambda = 0.25\n",
      "Current PID Loss 0.9714928799527517 lambda = 0.25\n",
      "Current PID Loss 0.9714923131360518 lambda = 0.25\n",
      "Current PID Loss 0.9714877928081824 lambda = 0.25\n",
      "Current PID Loss 0.9714883507679066 lambda = 0.25\n",
      "Current PID Loss 0.9714946314342449 lambda = 0.25\n",
      "Current PID Loss 0.9714910194685917 lambda = 0.25\n",
      "Current PID Loss 0.9714954919688933 lambda = 0.25\n",
      "Current PID Loss 0.9714889224003305 lambda = 0.25\n",
      "Current PID Loss 0.9714930701239399 lambda = 0.25\n",
      "Current PID Loss 0.9714918980715626 lambda = 0.25\n",
      "Current PID Loss 1.0122407832796407 lambda = 0.5\n",
      "Current PID Loss 0.9937771228184165 lambda = 0.5\n",
      "Current PID Loss 1.0092765881598296 lambda = 0.5\n",
      "Current PID Loss 1.044440004257968 lambda = 0.5\n",
      "Current PID Loss 1.0114438905264482 lambda = 0.5\n",
      "Current PID Loss 0.9858914890282787 lambda = 0.5\n",
      "Current PID Loss 0.9809693799967045 lambda = 0.5\n",
      "Current PID Loss 0.9825429196414832 lambda = 0.5\n",
      "Current PID Loss 0.9796200129450651 lambda = 0.5\n",
      "Current PID Loss 0.9854408841323056 lambda = 0.5\n",
      "Current PID Loss 0.9848962862583068 lambda = 0.5\n",
      "Current PID Loss 0.9971403089624292 lambda = 0.5\n",
      "Current PID Loss 0.9824559466354124 lambda = 0.5\n",
      "Current PID Loss 0.9984979667918286 lambda = 0.5\n",
      "Current PID Loss 0.9804324391105392 lambda = 0.5\n",
      "Current PID Loss 0.9816609925210643 lambda = 0.5\n",
      "Current PID Loss 0.9885948824340504 lambda = 0.5\n",
      "Current PID Loss 0.9800464320389413 lambda = 0.5\n",
      "Current PID Loss 0.9810937428239839 lambda = 0.5\n",
      "Current PID Loss 0.9801986010219068 lambda = 0.5\n",
      "Current PID Loss 0.9787778146205887 lambda = 0.5\n",
      "Current PID Loss 0.9779441093886482 lambda = 0.5\n",
      "Current PID Loss 0.9811649951928858 lambda = 0.5\n",
      "Current PID Loss 0.9795611470523851 lambda = 0.5\n",
      "Current PID Loss 0.9782848970540313 lambda = 0.5\n",
      "Current PID Loss 0.9780145373942257 lambda = 0.5\n",
      "Current PID Loss 0.9771017682630925 lambda = 0.5\n",
      "Current PID Loss 0.9759564096382999 lambda = 0.5\n",
      "Current PID Loss 0.9755894342743356 lambda = 0.5\n",
      "Current PID Loss 0.9740393391376353 lambda = 0.5\n",
      "Current PID Loss 0.9745197462256457 lambda = 0.5\n",
      "Current PID Loss 0.9741853179861186 lambda = 0.5\n",
      "Current PID Loss 0.9717358060585015 lambda = 0.5\n",
      "Current PID Loss 0.9694357981788 lambda = 0.5\n",
      "Current PID Loss 0.9706385937912421 lambda = 0.5\n",
      "Current PID Loss 0.9698197535329798 lambda = 0.5\n",
      "Current PID Loss 0.9681344110347394 lambda = 0.5\n",
      "Current PID Loss 0.9660674953959467 lambda = 0.5\n",
      "Current PID Loss 0.9651033832063015 lambda = 0.5\n",
      "Current PID Loss 0.962461441181194 lambda = 0.5\n",
      "Current PID Loss 0.9634593725023013 lambda = 0.5\n",
      "Current PID Loss 0.9619170367574006 lambda = 0.5\n",
      "Current PID Loss 0.959837667901128 lambda = 0.5\n",
      "Current PID Loss 0.9584484509461133 lambda = 0.5\n",
      "Current PID Loss 0.9556239914523128 lambda = 0.5\n",
      "Current PID Loss 0.9559075034620348 lambda = 0.5\n",
      "Current PID Loss 0.9544645185083016 lambda = 0.5\n",
      "Current PID Loss 0.9518605113827241 lambda = 0.5\n",
      "Current PID Loss 0.9515392707467139 lambda = 0.5\n",
      "Current PID Loss 0.9485649466845576 lambda = 0.5\n",
      "Current PID Loss 0.9481525330058591 lambda = 0.5\n",
      "Current PID Loss 0.9446603670106939 lambda = 0.5\n",
      "Current PID Loss 0.945045208031072 lambda = 0.5\n",
      "Current PID Loss 0.9413229100894359 lambda = 0.5\n",
      "Current PID Loss 0.9362424354575907 lambda = 0.5\n",
      "Current PID Loss 0.9365239251351051 lambda = 0.5\n",
      "Current PID Loss 0.9341561996150969 lambda = 0.5\n",
      "Current PID Loss 0.9289814752410852 lambda = 0.5\n",
      "Current PID Loss 0.9290315765479407 lambda = 0.5\n",
      "Current PID Loss 0.9231308523533484 lambda = 0.5\n",
      "Current PID Loss 0.9158130040797441 lambda = 0.5\n",
      "Current PID Loss 0.919271997405844 lambda = 0.5\n",
      "Current PID Loss 0.91360267224403 lambda = 0.5\n",
      "Current PID Loss 0.9071668720216269 lambda = 0.5\n",
      "Current PID Loss 0.9082122892617372 lambda = 0.5\n",
      "Current PID Loss 0.9021856273266773 lambda = 0.5\n",
      "Current PID Loss 0.8964737765925871 lambda = 0.5\n",
      "Current PID Loss 0.8976866074523309 lambda = 0.5\n",
      "Current PID Loss 0.8953663477315101 lambda = 0.5\n",
      "Current PID Loss 0.893374126656217 lambda = 0.5\n",
      "Current PID Loss 0.8928832299301788 lambda = 0.5\n",
      "Current PID Loss 0.8924108698356606 lambda = 0.5\n",
      "Current PID Loss 0.8923108655762532 lambda = 0.5\n",
      "Current PID Loss 0.8953125819964602 lambda = 0.5\n",
      "Current PID Loss 0.8938109735972684 lambda = 0.5\n",
      "Current PID Loss 0.8973554153555712 lambda = 0.5\n",
      "Current PID Loss 0.8925868677270512 lambda = 0.5\n",
      "Current PID Loss 0.8945643255139637 lambda = 0.5\n",
      "Current PID Loss 0.8927682984021601 lambda = 0.5\n",
      "Current PID Loss 0.892294568392809 lambda = 0.5\n",
      "Current PID Loss 0.8931124507501325 lambda = 0.5\n",
      "Current PID Loss 0.8913729866664365 lambda = 0.5\n",
      "Current PID Loss 0.8908342216269944 lambda = 0.5\n",
      "Current PID Loss 0.8931300060794936 lambda = 0.5\n",
      "Current PID Loss 0.8918361477677607 lambda = 0.5\n",
      "Current PID Loss 0.8908340852260165 lambda = 0.5\n",
      "Current PID Loss 0.8900604604519006 lambda = 0.5\n",
      "Current PID Loss 0.890040175333463 lambda = 0.5\n",
      "Current PID Loss 0.8893370079494909 lambda = 0.5\n",
      "Current PID Loss 0.8911004464175794 lambda = 0.5\n",
      "Current PID Loss 0.888091434237439 lambda = 0.5\n",
      "Current PID Loss 0.8865795115108628 lambda = 0.5\n",
      "Current PID Loss 0.8889004825266269 lambda = 0.5\n",
      "Current PID Loss 0.8859033608678837 lambda = 0.5\n",
      "Current PID Loss 0.8836617646856029 lambda = 0.5\n",
      "Current PID Loss 0.88418433992867 lambda = 0.5\n",
      "Current PID Loss 0.8819043407985765 lambda = 0.5\n",
      "Current PID Loss 0.8789013600165642 lambda = 0.5\n",
      "Current PID Loss 0.8801798576157263 lambda = 0.5\n",
      "Current PID Loss 0.8764686918521009 lambda = 0.5\n",
      "Current PID Loss 0.8729333608882275 lambda = 0.5\n",
      "Current PID Loss 0.8735431178787747 lambda = 0.5\n",
      "Current PID Loss 0.8687443579957221 lambda = 0.5\n",
      "Current PID Loss 0.8636921520319794 lambda = 0.5\n",
      "Current PID Loss 0.8692530056226756 lambda = 0.5\n",
      "Current PID Loss 0.8612521888544965 lambda = 0.5\n",
      "Current PID Loss 0.8550250302999632 lambda = 0.5\n",
      "Current PID Loss 0.8564142247112724 lambda = 0.5\n",
      "Current PID Loss 0.850848719099255 lambda = 0.5\n",
      "Current PID Loss 0.8436393950450565 lambda = 0.5\n",
      "Current PID Loss 0.8468634395038881 lambda = 0.5\n",
      "Current PID Loss 0.8399857903819086 lambda = 0.5\n",
      "Current PID Loss 0.8339844844451993 lambda = 0.5\n",
      "Current PID Loss 0.8327638309057428 lambda = 0.5\n",
      "Current PID Loss 0.8250234368231183 lambda = 0.5\n",
      "Current PID Loss 0.822421847605848 lambda = 0.5\n",
      "Current PID Loss 0.8127747514894725 lambda = 0.5\n",
      "Current PID Loss 0.8195019919971176 lambda = 0.5\n",
      "Current PID Loss 0.8090753065949567 lambda = 0.5\n",
      "Current PID Loss 0.801078469332436 lambda = 0.5\n",
      "Current PID Loss 0.7988311574185663 lambda = 0.5\n",
      "Current PID Loss 0.7891552809461284 lambda = 0.5\n",
      "Current PID Loss 0.7929462032721508 lambda = 0.5\n",
      "Current PID Loss 0.7828960804236543 lambda = 0.5\n",
      "Current PID Loss 0.7731301738166655 lambda = 0.5\n",
      "Current PID Loss 0.7798588926375577 lambda = 0.5\n",
      "Current PID Loss 0.7715948218499808 lambda = 0.5\n",
      "Current PID Loss 0.7654800914309752 lambda = 0.5\n",
      "Current PID Loss 0.7639020646881519 lambda = 0.5\n",
      "Current PID Loss 0.7579477930989633 lambda = 0.5\n",
      "Current PID Loss 0.7631362538394452 lambda = 0.5\n",
      "Current PID Loss 0.7528494373084323 lambda = 0.5\n",
      "Current PID Loss 0.7472918558423846 lambda = 0.5\n",
      "Current PID Loss 0.7591678268467756 lambda = 0.5\n",
      "Current PID Loss 0.7553123566853085 lambda = 0.5\n",
      "Current PID Loss 0.7482408485154115 lambda = 0.5\n",
      "Current PID Loss 0.7422971499583737 lambda = 0.5\n",
      "Current PID Loss 0.7356544779465078 lambda = 0.5\n",
      "Current PID Loss 0.7432860913558004 lambda = 0.5\n",
      "Current PID Loss 0.729857318192182 lambda = 0.5\n",
      "Current PID Loss 0.7210222807258151 lambda = 0.5\n",
      "Current PID Loss 0.7253321639231551 lambda = 0.5\n",
      "Current PID Loss 0.7140956002460879 lambda = 0.5\n",
      "Current PID Loss 0.7056314075240062 lambda = 0.5\n",
      "Current PID Loss 6.318901124557377 lambda = 0.5\n",
      "Current PID Loss 0.7285267378293385 lambda = 0.5\n",
      "Current PID Loss 0.7049081154125383 lambda = 0.5\n",
      "Current PID Loss 0.6957061988371429 lambda = 0.5\n",
      "Current PID Loss 0.7175606572576122 lambda = 0.5\n",
      "Current PID Loss 0.694465245761139 lambda = 0.5\n",
      "Current PID Loss 0.683566978490707 lambda = 0.5\n",
      "Current PID Loss 0.6784779340621181 lambda = 0.5\n",
      "Current PID Loss 4464918.938324401 lambda = 0.5\n",
      "Current PID Loss 0.683860348881119 lambda = 0.5\n",
      "Current PID Loss 0.6943977891049857 lambda = 0.5\n",
      "Current PID Loss 11.692781117080822 lambda = 0.5\n",
      "Current PID Loss 0.6873492564682531 lambda = 0.5\n",
      "Current PID Loss 0.7003311197076922 lambda = 0.5\n",
      "Current PID Loss 0.6836865967932803 lambda = 0.5\n",
      "Current PID Loss 0.693080803170663 lambda = 0.5\n",
      "Current PID Loss 0.6852917391621187 lambda = 0.5\n",
      "Current PID Loss 0.6828311273690953 lambda = 0.5\n",
      "Current PID Loss 1141.0449209377637 lambda = 0.5\n",
      "Current PID Loss 0.6836980464276898 lambda = 0.5\n",
      "Current PID Loss 0.7042077200613441 lambda = 0.5\n",
      "Current PID Loss 0.6833027780448627 lambda = 0.5\n",
      "Current PID Loss 0.6802836425144108 lambda = 0.5\n",
      "Current PID Loss 0.6817016471917099 lambda = 0.5\n",
      "Current PID Loss 0.7110681996090255 lambda = 0.5\n",
      "Current PID Loss 0.6822691427669586 lambda = 0.5\n",
      "Current PID Loss 0.6779738526724839 lambda = 0.5\n",
      "Current PID Loss 0.6764950378415224 lambda = 0.5\n",
      "Current PID Loss 0.679850589160993 lambda = 0.5\n",
      "Current PID Loss 0.6753826620314651 lambda = 0.5\n",
      "Current PID Loss 0.7403176638698241 lambda = 0.5\n",
      "Current PID Loss 0.7587475572926698 lambda = 0.5\n",
      "Current PID Loss 0.6773169317250868 lambda = 0.5\n",
      "Current PID Loss 0.6835785760604911 lambda = 0.5\n",
      "Current PID Loss 0.6733335517097567 lambda = 0.5\n",
      "Current PID Loss 0.6809702036227355 lambda = 0.5\n",
      "Current PID Loss 0.6758142834401124 lambda = 0.5\n",
      "Current PID Loss 0.6734106045585704 lambda = 0.5\n",
      "Current PID Loss 0.67471471137654 lambda = 0.5\n",
      "Current PID Loss 0.7606178689273823 lambda = 0.5\n",
      "Current PID Loss 0.6736754815301638 lambda = 0.5\n",
      "Current PID Loss 0.6773460658427218 lambda = 0.5\n",
      "Current PID Loss 0.6730344900525158 lambda = 0.5\n",
      "Current PID Loss 0.6747603521630475 lambda = 0.5\n",
      "Current PID Loss 0.6729447553268082 lambda = 0.5\n",
      "Current PID Loss 0.6767641862015559 lambda = 0.5\n",
      "Current PID Loss 0.672944074881845 lambda = 0.5\n",
      "Current PID Loss 0.673778945645749 lambda = 0.5\n",
      "Current PID Loss 0.6727107820200415 lambda = 0.5\n",
      "Current PID Loss 0.6734122008638825 lambda = 0.5\n",
      "Current PID Loss 0.6728371264954848 lambda = 0.5\n",
      "Current PID Loss 0.6736355935824009 lambda = 0.5\n",
      "Current PID Loss 0.672696682407494 lambda = 0.5\n",
      "Current PID Loss 0.6731562516040281 lambda = 0.5\n",
      "Current PID Loss 0.6727108392679387 lambda = 0.5\n",
      "Current PID Loss 0.6730118495721458 lambda = 0.5\n",
      "Current PID Loss 0.6727439473378302 lambda = 0.5\n",
      "Current PID Loss 0.6727579459277067 lambda = 0.5\n",
      "Current PID Loss 0.6726681135778845 lambda = 0.5\n",
      "Current PID Loss 0.6728982999391981 lambda = 0.5\n",
      "Current PID Loss 0.6726703100590166 lambda = 0.5\n",
      "Current PID Loss 0.6727920074699627 lambda = 0.5\n",
      "Current PID Loss 0.6726547454952515 lambda = 0.5\n",
      "Current PID Loss 0.672726119485108 lambda = 0.5\n",
      "Current PID Loss 0.6726521972773513 lambda = 0.5\n",
      "Current PID Loss 0.6727253919965214 lambda = 0.5\n",
      "Current PID Loss 0.6726585743864675 lambda = 0.5\n",
      "Current PID Loss 0.67271264291877 lambda = 0.5\n",
      "Current PID Loss 0.672648092392946 lambda = 0.5\n",
      "Current PID Loss 0.6726611347215437 lambda = 0.5\n",
      "Current PID Loss 0.6726467426269837 lambda = 0.5\n",
      "Current PID Loss 0.672682629398059 lambda = 0.5\n",
      "Current PID Loss 0.6726492667137681 lambda = 0.5\n",
      "Current PID Loss 0.6726521902746307 lambda = 0.5\n",
      "Current PID Loss 0.672671223295607 lambda = 0.5\n",
      "Current PID Loss 0.672644146120333 lambda = 0.5\n",
      "Current PID Loss 0.6726547527888704 lambda = 0.5\n",
      "Current PID Loss 0.6726509706635458 lambda = 0.5\n",
      "Current PID Loss 0.6726428966963478 lambda = 0.5\n",
      "Current PID Loss 0.6726547527888704 lambda = 0.5\n",
      "Current PID Loss 0.6726525710683556 lambda = 0.5\n",
      "Current PID Loss 0.6726492580082997 lambda = 0.5\n",
      "Current PID Loss 0.672647409468854 lambda = 0.5\n",
      "Current PID Loss 0.6726502268926402 lambda = 0.5\n",
      "Current PID Loss 0.6726438537674369 lambda = 0.5\n",
      "Current PID Loss 0.6726519493289916 lambda = 0.5\n",
      "Current PID Loss 0.6726512774882546 lambda = 0.5\n",
      "Current PID Loss 0.6726534713738684 lambda = 0.5\n",
      "Current PID Loss 0.6726412164906066 lambda = 0.5\n",
      "Current PID Loss 0.6726502359142973 lambda = 0.5\n",
      "Current PID Loss 0.6726491572036492 lambda = 0.5\n",
      "Current PID Loss 0.672650496794174 lambda = 0.5\n",
      "Current PID Loss 0.6726513023855963 lambda = 0.5\n",
      "Current PID Loss 0.672647863464899 lambda = 0.5\n",
      "Current PID Loss 0.6726415279316131 lambda = 0.5\n",
      "Current PID Loss 0.6726446599150442 lambda = 0.5\n",
      "Current PID Loss 0.6726508858807205 lambda = 0.5\n",
      "Current PID Loss 0.672647106101236 lambda = 0.5\n",
      "Current PID Loss 0.6726462613434986 lambda = 0.5\n",
      "Current PID Loss 0.672642106762607 lambda = 0.5\n",
      "Current PID Loss 0.6726408057878315 lambda = 0.5\n",
      "Current PID Loss 0.6726460920944414 lambda = 0.5\n",
      "Current PID Loss 0.6726505052216739 lambda = 0.5\n",
      "Current PID Loss 0.6726474226475797 lambda = 0.5\n",
      "Current PID Loss 0.6726454482605362 lambda = 0.5\n",
      "Current PID Loss 0.6726447025854759 lambda = 0.5\n",
      "Current PID Loss 0.6726457456309362 lambda = 0.5\n",
      "Current PID Loss 0.6726451038253842 lambda = 0.5\n",
      "Current PID Loss 0.6726421343676581 lambda = 0.5\n",
      "Current PID Loss 0.672651498218986 lambda = 0.5\n",
      "Current PID Loss 0.6726406642962839 lambda = 0.5\n",
      "Current PID Loss 0.6726484498912286 lambda = 0.5\n",
      "Current PID Loss 0.6726453852045642 lambda = 0.5\n",
      "Current PID Loss 0.6726407657144872 lambda = 0.5\n",
      "Current PID Loss 0.6726468421328012 lambda = 0.5\n",
      "Current PID Loss 0.6726485960796925 lambda = 0.5\n",
      "Current PID Loss 0.6726440318143158 lambda = 0.5\n",
      "Current PID Loss 0.6726469778884521 lambda = 0.5\n",
      "Current PID Loss 0.6726459154588493 lambda = 0.5\n",
      "Current PID Loss 0.6726431697112928 lambda = 0.5\n",
      "Current PID Loss 0.6726456474086664 lambda = 0.5\n",
      "Current PID Loss 0.672647732631569 lambda = 0.5\n",
      "Current PID Loss 0.6726461308637656 lambda = 0.5\n",
      "Current PID Loss 0.6726473017194967 lambda = 0.5\n",
      "Current PID Loss 0.6726459154588493 lambda = 0.5\n",
      "Current PID Loss 0.6726456995819845 lambda = 0.5\n",
      "Current PID Loss 0.6726431901874658 lambda = 0.5\n",
      "Current PID Loss 0.6726439902297101 lambda = 0.5\n",
      "Current PID Loss 0.6726455980649538 lambda = 0.5\n",
      "Current PID Loss 0.6726445820470169 lambda = 0.5\n",
      "Current PID Loss 0.6726436676100316 lambda = 0.5\n",
      "Current PID Loss 0.6726459154588493 lambda = 0.5\n",
      "Current PID Loss 0.6726502640833357 lambda = 0.5\n",
      "Current PID Loss 0.6726400789544452 lambda = 0.5\n",
      "Current PID Loss 0.6726402945953358 lambda = 0.5\n",
      "Current PID Loss 0.6726399094543394 lambda = 0.5\n",
      "Current PID Loss 0.6726461371671802 lambda = 0.5\n",
      "Current PID Loss 0.6726456534411998 lambda = 0.5\n",
      "Current PID Loss 0.6726404485565658 lambda = 0.5\n",
      "Current PID Loss 0.672644467652276 lambda = 0.5\n",
      "Current PID Loss 0.6726446830571923 lambda = 0.5\n",
      "Current PID Loss 0.6726400172747847 lambda = 0.5\n",
      "Current PID Loss 0.67264012509523 lambda = 0.5\n",
      "Current PID Loss 0.6726401867748905 lambda = 0.5\n",
      "Current PID Loss 0.6726445290959622 lambda = 0.5\n",
      "Current PID Loss 0.6726440980501555 lambda = 0.5\n",
      "Current PID Loss 0.6726439902297101 lambda = 0.5\n",
      "Current PID Loss 0.6726399094543394 lambda = 0.5\n",
      "Current PID Loss 0.6726398016338941 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726466145897461 lambda = 0.5\n",
      "Current PID Loss 0.6726398633135546 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726465067693008 lambda = 0.5\n",
      "Current PID Loss 0.6726398016338941 lambda = 0.5\n",
      "Current PID Loss 0.6726465067693008 lambda = 0.5\n",
      "Current PID Loss 0.6726397554931094 lambda = 0.5\n",
      "Current PID Loss 0.6726466145897461 lambda = 0.5\n",
      "Current PID Loss 0.6726397554931094 lambda = 0.5\n",
      "Current PID Loss 0.6726466145897461 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726465684489613 lambda = 0.5\n",
      "Current PID Loss 0.6726396938134489 lambda = 0.5\n",
      "Current PID Loss 0.6726466145897461 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.6726396476726642 lambda = 0.5\n",
      "Current PID Loss 0.3748852816237673 lambda = 0.75\n",
      "Current PID Loss 0.3762302011765508 lambda = 0.75\n",
      "Current PID Loss 0.37502120422483454 lambda = 0.75\n",
      "Current PID Loss 0.3762804078405159 lambda = 0.75\n",
      "Current PID Loss 0.3742864896998077 lambda = 0.75\n",
      "Current PID Loss 0.37496452610014386 lambda = 0.75\n",
      "Current PID Loss 0.3749475354484757 lambda = 0.75\n",
      "Current PID Loss 0.3734254595677111 lambda = 0.75\n",
      "Current PID Loss 0.372644463082545 lambda = 0.75\n",
      "Current PID Loss 0.3769776201667883 lambda = 0.75\n",
      "Current PID Loss 0.37398029976934055 lambda = 0.75\n",
      "Current PID Loss 0.3746119392044437 lambda = 0.75\n",
      "Current PID Loss 0.3729205986446439 lambda = 0.75\n",
      "Current PID Loss 0.3730364038607638 lambda = 0.75\n",
      "Current PID Loss 0.37206347967502806 lambda = 0.75\n",
      "Current PID Loss 0.3718365362230082 lambda = 0.75\n",
      "Current PID Loss 0.3703331781156596 lambda = 0.75\n",
      "Current PID Loss 0.36852084930513984 lambda = 0.75\n",
      "Current PID Loss 0.3723099360359713 lambda = 0.75\n",
      "Current PID Loss 0.368920275321427 lambda = 0.75\n",
      "Current PID Loss 0.37144841254535677 lambda = 0.75\n",
      "Current PID Loss 0.36733613001896054 lambda = 0.75\n",
      "Current PID Loss 0.36592961430096727 lambda = 0.75\n",
      "Current PID Loss 0.36397548144298353 lambda = 0.75\n",
      "Current PID Loss 0.3605785383557499 lambda = 0.75\n",
      "Current PID Loss 0.36774145769274535 lambda = 0.75\n",
      "Current PID Loss 0.3639366665741729 lambda = 0.75\n",
      "Current PID Loss 0.36165248539511236 lambda = 0.75\n",
      "Current PID Loss 0.3591757277863008 lambda = 0.75\n",
      "Current PID Loss 0.3655471039658097 lambda = 0.75\n",
      "Current PID Loss 0.35539741635263605 lambda = 0.75\n",
      "Current PID Loss 0.3520394705905192 lambda = 0.75\n",
      "Current PID Loss 0.3604242443154651 lambda = 0.75\n",
      "Current PID Loss 0.3668164898336623 lambda = 0.75\n",
      "Current PID Loss 0.35762487111590485 lambda = 0.75\n",
      "Current PID Loss 0.3590647053717596 lambda = 0.75\n",
      "Current PID Loss 0.3582111617161867 lambda = 0.75\n",
      "Current PID Loss 0.3550294657013471 lambda = 0.75\n",
      "Current PID Loss 0.36141462287655346 lambda = 0.75\n",
      "Current PID Loss 0.35377657678275776 lambda = 0.75\n",
      "Current PID Loss 0.35549447472076134 lambda = 0.75\n",
      "Current PID Loss 0.40027854340863606 lambda = 0.75\n",
      "Current PID Loss 0.3544847268086165 lambda = 0.75\n",
      "Current PID Loss 0.35703341933590055 lambda = 0.75\n",
      "Current PID Loss 0.3530463441262367 lambda = 0.75\n",
      "Current PID Loss 0.3559795818871957 lambda = 0.75\n",
      "Current PID Loss 0.35345814228649647 lambda = 0.75\n",
      "Current PID Loss 0.3542248765910957 lambda = 0.75\n",
      "Current PID Loss 0.3528768876709053 lambda = 0.75\n",
      "Current PID Loss 0.35149693698040463 lambda = 0.75\n",
      "Current PID Loss 0.350565208089476 lambda = 0.75\n",
      "Current PID Loss 0.35152350947215844 lambda = 0.75\n",
      "Current PID Loss 0.3499067669027456 lambda = 0.75\n",
      "Current PID Loss 0.348539500541878 lambda = 0.75\n",
      "Current PID Loss 0.3483005081468418 lambda = 0.75\n",
      "Current PID Loss 0.34663923330214186 lambda = 0.75\n",
      "Current PID Loss 0.3462358330325438 lambda = 0.75\n",
      "Current PID Loss 0.34456287357521564 lambda = 0.75\n",
      "Current PID Loss 0.34529074888279176 lambda = 0.75\n",
      "Current PID Loss 0.34116972880641516 lambda = 0.75\n",
      "Current PID Loss 0.3373086050061739 lambda = 0.75\n",
      "Current PID Loss 0.33758739688267014 lambda = 0.75\n",
      "Current PID Loss 0.33545314884518906 lambda = 0.75\n",
      "Current PID Loss 0.33982293993519763 lambda = 0.75\n",
      "Current PID Loss 0.33317631701021405 lambda = 0.75\n",
      "Current PID Loss 0.33373616678777024 lambda = 0.75\n",
      "Current PID Loss 0.3300000754772989 lambda = 0.75\n",
      "Current PID Loss 0.3269954806008374 lambda = 0.75\n",
      "Current PID Loss 0.32706085210738156 lambda = 0.75\n",
      "Current PID Loss 0.3267362746316761 lambda = 0.75\n",
      "Current PID Loss 0.35388160985934575 lambda = 0.75\n",
      "Current PID Loss 0.3223522467281703 lambda = 0.75\n",
      "Current PID Loss 0.3215801700671396 lambda = 0.75\n",
      "Current PID Loss 0.32355362865779747 lambda = 0.75\n",
      "Current PID Loss 0.3245441363207271 lambda = 0.75\n",
      "Current PID Loss 0.3407981919362648 lambda = 0.75\n",
      "Current PID Loss 0.3232130383453792 lambda = 0.75\n",
      "Current PID Loss 0.32319273991726777 lambda = 0.75\n",
      "Current PID Loss 0.32133719743500344 lambda = 0.75\n",
      "Current PID Loss 0.3217859110492104 lambda = 0.75\n",
      "Current PID Loss 0.3205123522725089 lambda = 0.75\n",
      "Current PID Loss 0.32071536978737614 lambda = 0.75\n",
      "Current PID Loss 0.32188067512727464 lambda = 0.75\n",
      "Current PID Loss 0.3217806384785755 lambda = 0.75\n",
      "Current PID Loss 0.32672249646966095 lambda = 0.75\n",
      "Current PID Loss 0.32069057768651416 lambda = 0.75\n",
      "Current PID Loss 0.3226227190545179 lambda = 0.75\n",
      "Current PID Loss 0.32081302200888945 lambda = 0.75\n",
      "Current PID Loss 0.32056910279820255 lambda = 0.75\n",
      "Current PID Loss 0.32030417390590316 lambda = 0.75\n",
      "Current PID Loss 0.3210528137256388 lambda = 0.75\n",
      "Current PID Loss 0.3200917803726652 lambda = 0.75\n",
      "Current PID Loss 0.3202836127642239 lambda = 0.75\n",
      "Current PID Loss 0.3208917878353849 lambda = 0.75\n",
      "Current PID Loss 0.3202703924681754 lambda = 0.75\n",
      "Current PID Loss 0.32062731399156785 lambda = 0.75\n",
      "Current PID Loss 0.32026328199340226 lambda = 0.75\n",
      "Current PID Loss 0.32026801105998953 lambda = 0.75\n",
      "Current PID Loss 0.32020972239066947 lambda = 0.75\n",
      "Current PID Loss 0.32015424358330474 lambda = 0.75\n",
      "Current PID Loss 0.3205473788199863 lambda = 0.75\n",
      "Current PID Loss 0.320103953328691 lambda = 0.75\n",
      "Current PID Loss 0.3203715745715195 lambda = 0.75\n",
      "Current PID Loss 0.3201144881041701 lambda = 0.75\n",
      "Current PID Loss 0.3202619433487366 lambda = 0.75\n",
      "Current PID Loss 0.32010959659418786 lambda = 0.75\n",
      "Current PID Loss 0.32021661285559444 lambda = 0.75\n",
      "Current PID Loss 0.3200922766306783 lambda = 0.75\n",
      "Current PID Loss 0.3201666444900386 lambda = 0.75\n",
      "Current PID Loss 0.32008583477309627 lambda = 0.75\n",
      "Current PID Loss 0.3201189911099666 lambda = 0.75\n",
      "Current PID Loss 0.3200883805615586 lambda = 0.75\n",
      "Current PID Loss 0.3201223694210318 lambda = 0.75\n",
      "Current PID Loss 0.3200846641407595 lambda = 0.75\n",
      "Current PID Loss 0.3201146151684734 lambda = 0.75\n",
      "Current PID Loss 0.32008373407655555 lambda = 0.75\n",
      "Current PID Loss 0.3200841126948061 lambda = 0.75\n",
      "Current PID Loss 0.3200904746770788 lambda = 0.75\n",
      "Current PID Loss 0.3200825876832922 lambda = 0.75\n",
      "Current PID Loss 0.3200936156368717 lambda = 0.75\n",
      "Current PID Loss 0.32008144262533444 lambda = 0.75\n",
      "Current PID Loss 0.32009297134748566 lambda = 0.75\n",
      "Current PID Loss 0.32008011474385456 lambda = 0.75\n",
      "Current PID Loss 0.3200898429604041 lambda = 0.75\n",
      "Current PID Loss 0.3200809913890521 lambda = 0.75\n",
      "Current PID Loss 0.3200810372094796 lambda = 0.75\n",
      "Current PID Loss 0.32008164385605736 lambda = 0.75\n",
      "Current PID Loss 0.320080782184029 lambda = 0.75\n",
      "Current PID Loss 0.3200805225578094 lambda = 0.75\n",
      "Current PID Loss 0.3200830613348955 lambda = 0.75\n",
      "Current PID Loss 0.32008108938775676 lambda = 0.75\n",
      "Current PID Loss 0.32008118864440654 lambda = 0.75\n",
      "Current PID Loss 0.3200785575436561 lambda = 0.75\n",
      "Current PID Loss 0.3200798027774794 lambda = 0.75\n",
      "Current PID Loss 0.32008145884537903 lambda = 0.75\n",
      "Current PID Loss 0.320081096001422 lambda = 0.75\n",
      "Current PID Loss 0.3200813433270108 lambda = 0.75\n",
      "Current PID Loss 0.32007977255050607 lambda = 0.75\n",
      "Current PID Loss 0.3200815192566556 lambda = 0.75\n",
      "Current PID Loss 0.32008211152859234 lambda = 0.75\n",
      "Current PID Loss 0.3200807207642251 lambda = 0.75\n",
      "Current PID Loss 0.32007949720574935 lambda = 0.75\n",
      "Current PID Loss 0.32008001229117694 lambda = 0.75\n",
      "Current PID Loss 0.3200809835018396 lambda = 0.75\n",
      "Current PID Loss 0.3200785970378982 lambda = 0.75\n",
      "Current PID Loss 0.3200811386849618 lambda = 0.75\n",
      "Current PID Loss 0.3200804198145834 lambda = 0.75\n",
      "Current PID Loss 0.32008079268183187 lambda = 0.75\n",
      "Current PID Loss 0.3200787764672945 lambda = 0.75\n",
      "Current PID Loss 0.3200801604681997 lambda = 0.75\n",
      "Current PID Loss 0.32008106137178566 lambda = 0.75\n",
      "Current PID Loss 0.32007983693535896 lambda = 0.75\n",
      "Current PID Loss 0.3200783310036624 lambda = 0.75\n",
      "Current PID Loss 0.3200803126934074 lambda = 0.75\n",
      "Current PID Loss 0.3200792537736848 lambda = 0.75\n",
      "Current PID Loss 0.3200800440971752 lambda = 0.75\n",
      "Current PID Loss 0.3200801388484717 lambda = 0.75\n",
      "Current PID Loss 0.3200799131871036 lambda = 0.75\n",
      "Current PID Loss 0.32008066422004666 lambda = 0.75\n",
      "Current PID Loss 0.32008104099763784 lambda = 0.75\n",
      "Current PID Loss 0.3200809827489857 lambda = 0.75\n",
      "Current PID Loss 0.3200806458288985 lambda = 0.75\n",
      "Current PID Loss 0.32007913669771587 lambda = 0.75\n",
      "Current PID Loss 0.32007851741039406 lambda = 0.75\n",
      "Current PID Loss 0.3200790737200541 lambda = 0.75\n",
      "Current PID Loss 0.3200807791492841 lambda = 0.75\n",
      "Current PID Loss 0.32008051871866716 lambda = 0.75\n",
      "Current PID Loss 0.3200784580075725 lambda = 0.75\n",
      "Current PID Loss 0.3200785441358366 lambda = 0.75\n",
      "Current PID Loss 0.32007876656770884 lambda = 0.75\n",
      "Current PID Loss 0.32008089064432427 lambda = 0.75\n",
      "Current PID Loss 0.320081119275088 lambda = 0.75\n",
      "Current PID Loss 0.3200812655049239 lambda = 0.75\n",
      "Current PID Loss 0.320078575348269 lambda = 0.75\n",
      "Current PID Loss 0.3200805053159056 lambda = 0.75\n",
      "Current PID Loss 0.3200783905596639 lambda = 0.75\n",
      "Current PID Loss 0.3200812214497544 lambda = 0.75\n",
      "Current PID Loss 0.3200782209431394 lambda = 0.75\n",
      "Current PID Loss 0.3200780670983281 lambda = 0.75\n",
      "Current PID Loss 0.3200802520039893 lambda = 0.75\n",
      "Current PID Loss 0.3200789370769882 lambda = 0.75\n",
      "Current PID Loss 0.3200810597190865 lambda = 0.75\n",
      "Current PID Loss 0.3200786136156524 lambda = 0.75\n",
      "Current PID Loss 0.3200819816387727 lambda = 0.75\n",
      "Current PID Loss 0.32007854440447525 lambda = 0.75\n",
      "Current PID Loss 0.3200784912358924 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200814301463057 lambda = 0.75\n",
      "Current PID Loss 0.32007844427000903 lambda = 0.75\n",
      "Current PID Loss 0.32007838267380734 lambda = 0.75\n",
      "Current PID Loss 0.32008139079539777 lambda = 0.75\n",
      "Current PID Loss 0.3200786136156524 lambda = 0.75\n",
      "Current PID Loss 0.32007832134848674 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200786136156524 lambda = 0.75\n",
      "Current PID Loss 0.32007848307915465 lambda = 0.75\n",
      "Current PID Loss 0.32007855229033183 lambda = 0.75\n",
      "Current PID Loss 0.32007855229033183 lambda = 0.75\n",
      "Current PID Loss 0.3200783905596639 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200783386186378 lambda = 0.75\n",
      "Current PID Loss 0.3200783905596639 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200783905596639 lambda = 0.75\n",
      "Current PID Loss 0.3200783386186378 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200782694074607 lambda = 0.75\n",
      "Current PID Loss 0.3200779538319814 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200778846208043 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200781768879699 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.3200777921013135 lambda = 0.75\n",
      "Current PID Loss 0.019729778800823054 lambda = 1.0\n",
      "Current PID Loss 0.02115549330233779 lambda = 1.0\n",
      "Current PID Loss 0.019841680963396925 lambda = 1.0\n",
      "Current PID Loss 0.020254705152157878 lambda = 1.0\n",
      "Current PID Loss 0.02064930442269601 lambda = 1.0\n",
      "Current PID Loss 0.025268100677790718 lambda = 1.0\n",
      "Current PID Loss 0.019786015577584627 lambda = 1.0\n",
      "Current PID Loss 0.019295756093020346 lambda = 1.0\n",
      "Current PID Loss 0.019379059188688257 lambda = 1.0\n",
      "Current PID Loss 0.01962136349424807 lambda = 1.0\n",
      "Current PID Loss 0.019856621149813715 lambda = 1.0\n",
      "Current PID Loss 0.019564196587197238 lambda = 1.0\n",
      "Current PID Loss 0.01929374835660621 lambda = 1.0\n",
      "Current PID Loss 0.019414586983716182 lambda = 1.0\n",
      "Current PID Loss 0.019336221539733505 lambda = 1.0\n",
      "Current PID Loss 0.019502592066857862 lambda = 1.0\n",
      "Current PID Loss 0.019054814001669394 lambda = 1.0\n",
      "Current PID Loss 0.019105784197327964 lambda = 1.0\n",
      "Current PID Loss 0.019449971142567107 lambda = 1.0\n",
      "Current PID Loss 0.01921284032777245 lambda = 1.0\n",
      "Current PID Loss 0.0193238843580829 lambda = 1.0\n",
      "Current PID Loss 0.019184522177005926 lambda = 1.0\n",
      "Current PID Loss 0.018953689252232092 lambda = 1.0\n",
      "Current PID Loss 0.01884227413947716 lambda = 1.0\n",
      "Current PID Loss 0.019062233566315544 lambda = 1.0\n",
      "Current PID Loss 0.019102045361803887 lambda = 1.0\n",
      "Current PID Loss 0.01889177254737054 lambda = 1.0\n",
      "Current PID Loss 0.01907728318258926 lambda = 1.0\n",
      "Current PID Loss 0.01893697722360521 lambda = 1.0\n",
      "Current PID Loss 0.018852499448146407 lambda = 1.0\n",
      "Current PID Loss 0.01862887870422432 lambda = 1.0\n",
      "Current PID Loss 0.018517997214506476 lambda = 1.0\n",
      "Current PID Loss 0.018895987484718206 lambda = 1.0\n",
      "Current PID Loss 0.018755010104592136 lambda = 1.0\n",
      "Current PID Loss 0.018815546757444294 lambda = 1.0\n",
      "Current PID Loss 0.018531281977324437 lambda = 1.0\n",
      "Current PID Loss 0.01859940813206241 lambda = 1.0\n",
      "Current PID Loss 0.018747350890998632 lambda = 1.0\n",
      "Current PID Loss 0.01846787698021832 lambda = 1.0\n",
      "Current PID Loss 0.018632420114140236 lambda = 1.0\n",
      "Current PID Loss 0.018461077432540496 lambda = 1.0\n",
      "Current PID Loss 0.018689558249941614 lambda = 1.0\n",
      "Current PID Loss 0.01834047884607916 lambda = 1.0\n",
      "Current PID Loss 0.018354124258732864 lambda = 1.0\n",
      "Current PID Loss 0.01830226982177903 lambda = 1.0\n",
      "Current PID Loss 0.018340287490317925 lambda = 1.0\n",
      "Current PID Loss 0.01815705452534991 lambda = 1.0\n",
      "Current PID Loss 0.018046483277389745 lambda = 1.0\n",
      "Current PID Loss 0.018258696069844944 lambda = 1.0\n",
      "Current PID Loss 0.018017668809703038 lambda = 1.0\n",
      "Current PID Loss 0.018081429984388907 lambda = 1.0\n",
      "Current PID Loss 0.01789650828186841 lambda = 1.0\n",
      "Current PID Loss 0.017832368961106335 lambda = 1.0\n",
      "Current PID Loss 0.017755553391618165 lambda = 1.0\n",
      "Current PID Loss 0.017763452472998657 lambda = 1.0\n",
      "Current PID Loss 0.018112703648026314 lambda = 1.0\n",
      "Current PID Loss 0.01785031088800536 lambda = 1.0\n",
      "Current PID Loss 0.017546294645899612 lambda = 1.0\n",
      "Current PID Loss 0.01739429808463009 lambda = 1.0\n",
      "Current PID Loss 0.01764363911251971 lambda = 1.0\n",
      "Current PID Loss 0.017367896080884605 lambda = 1.0\n",
      "Current PID Loss 0.017296281352671992 lambda = 1.0\n",
      "Current PID Loss 0.01712001455826112 lambda = 1.0\n",
      "Current PID Loss 0.01704482642216779 lambda = 1.0\n",
      "Current PID Loss 0.017043865939446783 lambda = 1.0\n",
      "Current PID Loss 0.01893966256477144 lambda = 1.0\n",
      "Current PID Loss 0.01688532511383712 lambda = 1.0\n",
      "Current PID Loss 0.017406570519184657 lambda = 1.0\n",
      "Current PID Loss 0.017475895565036075 lambda = 1.0\n",
      "Current PID Loss 0.017021009478031486 lambda = 1.0\n",
      "Current PID Loss 0.016972999810479273 lambda = 1.0\n",
      "Current PID Loss 0.017267277335982816 lambda = 1.0\n",
      "Current PID Loss 0.016859463286788397 lambda = 1.0\n",
      "Current PID Loss 0.017031714541221373 lambda = 1.0\n",
      "Current PID Loss 0.016880899478085562 lambda = 1.0\n",
      "Current PID Loss 0.01723678112702311 lambda = 1.0\n",
      "Current PID Loss 0.016858128363083133 lambda = 1.0\n",
      "Current PID Loss 0.017200239093738284 lambda = 1.0\n",
      "Current PID Loss 0.016818559958029625 lambda = 1.0\n",
      "Current PID Loss 0.016850608645527836 lambda = 1.0\n",
      "Current PID Loss 0.016905297191450668 lambda = 1.0\n",
      "Current PID Loss 0.016834480258924762 lambda = 1.0\n",
      "Current PID Loss 0.01685895846931154 lambda = 1.0\n",
      "Current PID Loss 0.016828235145910225 lambda = 1.0\n",
      "Current PID Loss 0.01681228313745347 lambda = 1.0\n",
      "Current PID Loss 0.01686804647267063 lambda = 1.0\n",
      "Current PID Loss 0.016880439116899 lambda = 1.0\n",
      "Current PID Loss 0.01681682945108182 lambda = 1.0\n",
      "Current PID Loss 0.016858025540208092 lambda = 1.0\n",
      "Current PID Loss 0.016813469201400536 lambda = 1.0\n",
      "Current PID Loss 0.01681470586696036 lambda = 1.0\n",
      "Current PID Loss 0.016808614943473268 lambda = 1.0\n",
      "Current PID Loss 0.016831481507987824 lambda = 1.0\n",
      "Current PID Loss 0.016825044349621388 lambda = 1.0\n",
      "Current PID Loss 0.01680625005907154 lambda = 1.0\n",
      "Current PID Loss 0.01683374464072893 lambda = 1.0\n",
      "Current PID Loss 0.01680530410065499 lambda = 1.0\n",
      "Current PID Loss 0.016803898886404673 lambda = 1.0\n",
      "Current PID Loss 0.016818107326473784 lambda = 1.0\n",
      "Current PID Loss 0.016813887099273367 lambda = 1.0\n",
      "Current PID Loss 0.016804397854601326 lambda = 1.0\n",
      "Current PID Loss 0.016821338450988516 lambda = 1.0\n",
      "Current PID Loss 0.016802812717945155 lambda = 1.0\n",
      "Current PID Loss 0.01680766893512173 lambda = 1.0\n",
      "Current PID Loss 0.01680319352831191 lambda = 1.0\n",
      "Current PID Loss 0.01680333833881385 lambda = 1.0\n",
      "Current PID Loss 0.016803262507798854 lambda = 1.0\n",
      "Current PID Loss 0.01681322588768415 lambda = 1.0\n",
      "Current PID Loss 0.016801469636918413 lambda = 1.0\n",
      "Current PID Loss 0.01680333837535164 lambda = 1.0\n",
      "Current PID Loss 0.01680201375742271 lambda = 1.0\n",
      "Current PID Loss 0.016804105484849893 lambda = 1.0\n",
      "Current PID Loss 0.016802547963498195 lambda = 1.0\n",
      "Current PID Loss 0.016801584740015404 lambda = 1.0\n",
      "Current PID Loss 0.01680365020524071 lambda = 1.0\n",
      "Current PID Loss 0.016801435436037352 lambda = 1.0\n",
      "Current PID Loss 0.01680157395737667 lambda = 1.0\n",
      "Current PID Loss 0.016800204865401534 lambda = 1.0\n",
      "Current PID Loss 0.016799809294353346 lambda = 1.0\n",
      "Current PID Loss 0.016801498260476107 lambda = 1.0\n",
      "Current PID Loss 0.01680061169478545 lambda = 1.0\n",
      "Current PID Loss 0.01680115927408562 lambda = 1.0\n",
      "Current PID Loss 0.01680062182408259 lambda = 1.0\n",
      "Current PID Loss 0.016800316264963845 lambda = 1.0\n",
      "Current PID Loss 0.01680127865374563 lambda = 1.0\n",
      "Current PID Loss 0.016799911212075597 lambda = 1.0\n",
      "Current PID Loss 0.01680135617456608 lambda = 1.0\n",
      "Current PID Loss 0.016799716327617033 lambda = 1.0\n",
      "Current PID Loss 0.016799872574979273 lambda = 1.0\n",
      "Current PID Loss 0.016800164783074445 lambda = 1.0\n",
      "Current PID Loss 0.016799450781976012 lambda = 1.0\n",
      "Current PID Loss 0.016798814559776044 lambda = 1.0\n",
      "Current PID Loss 0.016798412613731075 lambda = 1.0\n",
      "Current PID Loss 0.016799702164206204 lambda = 1.0\n",
      "Current PID Loss 0.016800053956563775 lambda = 1.0\n",
      "Current PID Loss 0.016799224122037774 lambda = 1.0\n",
      "Current PID Loss 0.016798958341379896 lambda = 1.0\n",
      "Current PID Loss 0.016798695360169646 lambda = 1.0\n",
      "Current PID Loss 0.016798534542502106 lambda = 1.0\n",
      "Current PID Loss 0.01679799780064879 lambda = 1.0\n",
      "Current PID Loss 0.01679753169591879 lambda = 1.0\n",
      "Current PID Loss 0.01679850040802484 lambda = 1.0\n",
      "Current PID Loss 0.01679817653491397 lambda = 1.0\n",
      "Current PID Loss 0.016798593924197677 lambda = 1.0\n",
      "Current PID Loss 0.016798186807526268 lambda = 1.0\n",
      "Current PID Loss 0.016798624547153286 lambda = 1.0\n",
      "Current PID Loss 0.016798163785773583 lambda = 1.0\n",
      "Current PID Loss 0.01679855670808819 lambda = 1.0\n",
      "Current PID Loss 0.016798281453881438 lambda = 1.0\n",
      "Current PID Loss 0.016798451420368912 lambda = 1.0\n",
      "Current PID Loss 0.016797965114009644 lambda = 1.0\n",
      "Current PID Loss 0.016798361144093547 lambda = 1.0\n",
      "Current PID Loss 0.01679792442057656 lambda = 1.0\n",
      "Current PID Loss 0.016797963325261667 lambda = 1.0\n",
      "Current PID Loss 0.01679740474098451 lambda = 1.0\n",
      "Current PID Loss 0.016797173395812465 lambda = 1.0\n",
      "Current PID Loss 0.016797791896926834 lambda = 1.0\n",
      "Current PID Loss 0.016797948373727337 lambda = 1.0\n",
      "Current PID Loss 0.016798047981833965 lambda = 1.0\n",
      "Current PID Loss 0.01679728229934591 lambda = 1.0\n",
      "Current PID Loss 0.016797425158973485 lambda = 1.0\n",
      "Current PID Loss 0.016797463505965282 lambda = 1.0\n",
      "Current PID Loss 0.016797969920049238 lambda = 1.0\n",
      "Current PID Loss 0.016797610924140585 lambda = 1.0\n",
      "Current PID Loss 0.016796937870220376 lambda = 1.0\n",
      "Current PID Loss 0.016797726843976336 lambda = 1.0\n",
      "Current PID Loss 0.016797449467519876 lambda = 1.0\n",
      "Current PID Loss 0.0167975811365254 lambda = 1.0\n",
      "Current PID Loss 0.016797859253603227 lambda = 1.0\n",
      "Current PID Loss 0.016797736722298864 lambda = 1.0\n",
      "Current PID Loss 0.01679781976716335 lambda = 1.0\n",
      "Current PID Loss 0.016797951220954437 lambda = 1.0\n",
      "Current PID Loss 0.016797543136408164 lambda = 1.0\n",
      "Current PID Loss 0.016797484840634924 lambda = 1.0\n",
      "Current PID Loss 0.016797684653444916 lambda = 1.0\n",
      "Current PID Loss 0.016797364175405286 lambda = 1.0\n",
      "Current PID Loss 0.016797589705708 lambda = 1.0\n",
      "Current PID Loss 0.01679755867606776 lambda = 1.0\n",
      "Current PID Loss 0.016797834355179946 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.016797714710032333 lambda = 1.0\n",
      "Current PID Loss 0.016797189985423434 lambda = 1.0\n",
      "Current PID Loss 0.016797397503226737 lambda = 1.0\n",
      "Current PID Loss 0.016797313344744463 lambda = 1.0\n",
      "Current PID Loss 0.016797861980129492 lambda = 1.0\n",
      "Current PID Loss 0.016797608199955953 lambda = 1.0\n",
      "Current PID Loss 0.01679729900173091 lambda = 1.0\n",
      "Current PID Loss 0.01679774462652557 lambda = 1.0\n",
      "Current PID Loss 0.01679774462652557 lambda = 1.0\n",
      "Current PID Loss 0.016797499518450356 lambda = 1.0\n",
      "Current PID Loss 0.016797660738924445 lambda = 1.0\n",
      "Current PID Loss 0.0167973925590654 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.016797660738924445 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.016797528985635017 lambda = 1.0\n",
      "Current PID Loss 0.016797876379815 lambda = 1.0\n",
      "Current PID Loss 0.016796975955099717 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.016798175908306685 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.016797660738924445 lambda = 1.0\n",
      "Current PID Loss 0.01679756845735492 lambda = 1.0\n",
      "Current PID Loss 0.016797244134958766 lambda = 1.0\n",
      "Current PID Loss 0.01679796026741613 lambda = 1.0\n",
      "Current PID Loss 0.016797660738924445 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.01679745977584932 lambda = 1.0\n",
      "Current PID Loss 0.016797367494279795 lambda = 1.0\n",
      "Current PID Loss 0.016797675416739873 lambda = 1.0\n",
      "Current PID Loss 0.01679702849406821 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.016797367494279795 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.016797244134958766 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.016797244134958766 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.01679715185338924 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.01679715185338924 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.016798175908306685 lambda = 1.0\n",
      "Current PID Loss 0.01679715185338924 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.016796936212498684 lambda = 1.0\n",
      "Current PID Loss 0.01679715185338924 lambda = 1.0\n",
      "Current PID Loss 0.016797784098245474 lambda = 1.0\n",
      "Current PID Loss 0.01679702849406821 lambda = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_107_108_results = createGroupedFrontier([107, 108]) # Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
